"use strict";(self.webpackChunkstacks=self.webpackChunkstacks||[]).push([[7214],{3905:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return f}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=p(n),m=r,f=c["".concat(s,".").concat(m)]||c[m]||u[m]||i;return n?a.createElement(f,o(o({ref:t},d),{},{components:n})):a.createElement(f,o({ref:t},d))}));function f(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[c]="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},741:function(e,t,n){n.r(t),n.d(t,{contentTitle:function(){return s},default:function(){return m},frontMatter:function(){return l},metadata:function(){return p},toc:function(){return d}});var a=n(7462),r=n(3366),i=(n(7294),n(3905)),o=["components"],l={id:"etl_pipelines_deployment_azure",title:"ETL Pipeline Deployment",sidebar_label:"ETL Pipeline Deployment",hide_title:!1,hide_table_of_contents:!1,description:"Data pipelines development & deployment",keywords:["datastacks","data","python","etl","cli","azure","template"]},s=void 0,p={unversionedId:"workloads/azure/data/getting_started/etl_pipelines_deployment_azure",id:"workloads/azure/data/getting_started/etl_pipelines_deployment_azure",isDocsHomePage:!1,title:"ETL Pipeline Deployment",description:"Data pipelines development & deployment",source:"@site/docs/workloads/azure/data/getting_started/etl_pipelines_deployment_azure.md",sourceDirName:"workloads/azure/data/getting_started",slug:"/workloads/azure/data/getting_started/etl_pipelines_deployment_azure",permalink:"/docs/workloads/azure/data/getting_started/etl_pipelines_deployment_azure",tags:[],version:"current",frontMatter:{id:"etl_pipelines_deployment_azure",title:"ETL Pipeline Deployment",sidebar_label:"ETL Pipeline Deployment",hide_title:!1,hide_table_of_contents:!1,description:"Data pipelines development & deployment",keywords:["datastacks","data","python","etl","cli","azure","template"]},sidebar:"docs",previous:{title:"Infrastructure Deployment",permalink:"/docs/workloads/azure/data/getting_started/core_data_platform_deployment_azure"},next:{title:"Testing",permalink:"/docs/workloads/azure/data/getting_started/testing_data_azure"}},d=[{value:"Create feature branch",id:"create-feature-branch",children:[],level:2},{value:"Prepare the Datastacks config file",id:"prepare-the-datastacks-config-file",children:[],level:2},{value:"Generate project artifacts using Datastacks",id:"generate-project-artifacts-using-datastacks",children:[],level:2},{value:"Deploy new workload in non-production environment",id:"deploy-new-workload-in-non-production-environment",children:[],level:2},{value:"Deploy new workload in further environments",id:"deploy-new-workload-in-further-environments",children:[],level:2}],c={toc:d},u="wrapper";function m(e){var t=e.components,n=(0,r.Z)(e,o);return(0,i.kt)(u,(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("hr",null),(0,i.kt)("p",null,"This section provides an overview of generating a new ETL ingest pipeline / data engineering workload and deploying it into a Stacks Data Platform, using the ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/etl_pipelines/datastacks"},"Datastacks")," utility.\nThis aligns to the workflow shown in the ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/architecture/architecture_data_azure#data-engineering-workloads"},"deployment architecture")," section.\nIt assumes all prerequisites are in place, including:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"A ",(0,i.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/core_data_platform_deployment_azure"},"deployed Stacks data platform")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/dev_quickstart_data_azure"},"Development environment set up"))),(0,i.kt)("p",null,"This process will deploy the following resources into the project:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Azure Data Factory resources (defined in Terraform / ARM)",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Linked service"),(0,i.kt)("li",{parentName:"ul"},"Dataset"),(0,i.kt)("li",{parentName:"ul"},"Pipeline"))),(0,i.kt)("li",{parentName:"ul"},"Data ingest config files (JSON)"),(0,i.kt)("li",{parentName:"ul"},"Azure DevOps CI/CD pipeline (YAML)"),(0,i.kt)("li",{parentName:"ul"},"(optional) Spark jobs for data quality tests (Python)"),(0,i.kt)("li",{parentName:"ul"},"Template unit tests (Python)"),(0,i.kt)("li",{parentName:"ul"},"Template end-to-end tests (Python, Behave)")),(0,i.kt)("h2",{id:"create-feature-branch"},"Create feature branch"),(0,i.kt)("p",null,"Before creating a new workload using Datastacks, open the project locally and create a new branch for the workload being created, e.g.:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"git checkout -b feat/my-new-data-pipeline\n")),(0,i.kt)("h2",{id:"prepare-the-datastacks-config-file"},"Prepare the Datastacks config file"),(0,i.kt)("p",null,"Datastacks requires a config file for generating a new ingest workload. This config file should be a yaml file. A sample config file is included in the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/amido/stacks-azure-data/tree/main/de_templates"},"de_templates")," folder."),(0,i.kt)("p",null,"Using the sample config file as a starting point, create a new config file and populate the values relevant to your new ingest pipeline, e.g."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'# `dataset_name` parameter is used to determine names of the following ADF resources:\n# - pipeline: Ingest_<dataset_name>\n# - dataset: ds_<dataset_name>\n# - linked service: ls_<dataset_name>\ndataset_name: AzureSql_MyNewExample\npipeline_description: "Ingest from demo Azure SQL database using ingest config file."\ndata_source_type: azure_sql\n\nkey_vault_linked_service_name: ls_KeyVault\ndata_source_password_key_vault_secret_name: sql-password\ndata_source_connection_string_variable_name: sql_connection\n\n# Azure DevOps configurations\n\nado_variable_groups_nonprod:\n  - amido-stacks-de-pipeline-nonprod\n  - amido-stacks-infra-credentials-nonprod\n  - stacks-credentials-nonprod-kv\n\nado_variable_groups_prod:\n  - amido-stacks-de-pipeline-prod\n  - amido-stacks-infra-credentials-prod\n  - stacks-credentials-prod-kv\n\n# Datalake containers\n\nbronze_container: raw\nsilver_container: staging\ngold_container: curated\n')),(0,i.kt)("h2",{id:"generate-project-artifacts-using-datastacks"},"Generate project artifacts using Datastacks"),(0,i.kt)("p",null,"Use the Datastacks CLI to generate the artifacts for the new workload, using the prepared config file (replacing ",(0,i.kt)("inlineCode",{parentName:"p"},"path_to_config_file/my_config.yaml")," with the appropriate path). Note, a workload with Data Quality steps requires a data platform with a Databricks workspace:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'# Initiate Datastacks using poetry:\npoetry run datastacks\n\n# Generate resources for an ingest pipeline (without Data Quality steps)\ndatastacks generate ingest --config="path_to_config_file/my_config.yaml"\n\n# Generate resources for an ingest pipeline (with added Data Quality steps)\ndatastacks generate ingest --config="path_to_config_file/my_config.yaml" --data-quality\n')),(0,i.kt)("p",null,"This should add new project artifacts for the workload under ",(0,i.kt)("inlineCode",{parentName:"p"},"de_workloads/ingest"),", based on the ingest workload templates. Review the resources that have been generated."),(0,i.kt)("p",null,"Once reviewed, commit the resources and push the branch to the remote project repository."),(0,i.kt)("h2",{id:"deploy-new-workload-in-non-production-environment"},"Deploy new workload in non-production environment"),(0,i.kt)("p",null,"The generated workload will contain a YAML file containing a template Azure DevOps CI/CD pipeline for the workload, named ",(0,i.kt)("inlineCode",{parentName:"p"},"de-ingest-ado-pipeline.yaml"),". This can be added as the definition for a new pipeline in Azure DevOps."),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Sign-in to your Azure DevOps organization and go to your project"),(0,i.kt)("li",{parentName:"ol"},"Go to Pipelines, and then select New pipeline"),(0,i.kt)("li",{parentName:"ol"},"Name the new pipeline to match the name of your new workload, e.g. ",(0,i.kt)("inlineCode",{parentName:"li"},"de-ingest-azuresql-mynewexample")),(0,i.kt)("li",{parentName:"ol"},"For the pipeline definition, specify the YAML file in the project repository feature branch (e.g. ",(0,i.kt)("inlineCode",{parentName:"li"},"de-ingest-ado-pipeline.yaml"),") and save"),(0,i.kt)("li",{parentName:"ol"},"The new pipeline will require access to any Azure DevOps pipeline variable groups specified in the config file. Under each variable group, go to 'Pipeline permissions' and add the new pipeline."),(0,i.kt)("li",{parentName:"ol"},"Run the new pipeline")),(0,i.kt)("p",null,"Running this pipeline in Azure DevOps will deploy the artifacts into the non-production (nonprod) environment and run tests. If successful, the generated resources will now be available in the nonprod Stacks environment."),(0,i.kt)("p",null,"Continue to make any further amendments required to the new workload, re-running the pipeline as required - for example:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Update the (",(0,i.kt)("inlineCode",{parentName:"li"},"ingest_config"),") file with details of the data required from the new data source"),(0,i.kt)("li",{parentName:"ul"},"(If including DQ) update the (",(0,i.kt)("inlineCode",{parentName:"li"},"ingest_dq"),") file with details of data quality checks required on the data"),(0,i.kt)("li",{parentName:"ul"},"Update end-to-end tests, to ensure test coverage of the new data pipeline")),(0,i.kt)("h2",{id:"deploy-new-workload-in-further-environments"},"Deploy new workload in further environments"),(0,i.kt)("p",null,"It is recommended in any Stacks data platform that processes for deploying and releasing to further should be agreed and documented, ensuring sufficient review and quality assurance of any new workloads. The template CI/CD pipelines provided are based upon two platform environments (nonprod and prod) - but these may be amended depending upon the specific requirements of your project and organisation."),(0,i.kt)("p",null,"In the example pipeline templates:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Deployment to the non-production (nonprod) environment is triggered on a feature branch when a pull request is open"),(0,i.kt)("li",{parentName:"ul"},"Deployment to the production (prod) environment is triggered on merging to the ",(0,i.kt)("inlineCode",{parentName:"li"},"main")," branch, followed by manual approval of the release step.")))}m.isMDXComponent=!0}}]);