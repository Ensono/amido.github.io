"use strict";(self.webpackChunkstacks=self.webpackChunkstacks||[]).push([[910],{3905:function(t,e,a){a.d(e,{Zo:function(){return s},kt:function(){return g}});var n=a(7294);function r(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function l(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,n)}return a}function i(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?l(Object(a),!0).forEach((function(e){r(t,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(a,e))}))}return t}function o(t,e){if(null==t)return{};var a,n,r=function(t,e){if(null==t)return{};var a,n,r={},l=Object.keys(t);for(n=0;n<l.length;n++)a=l[n],e.indexOf(a)>=0||(r[a]=t[a]);return r}(t,e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(t);for(n=0;n<l.length;n++)a=l[n],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(r[a]=t[a])}return r}var d=n.createContext({}),p=function(t){var e=n.useContext(d),a=e;return t&&(a="function"==typeof t?t(e):i(i({},e),t)),a},s=function(t){var e=p(t.components);return n.createElement(d.Provider,{value:e},t.children)},u="mdxType",m={inlineCode:"code",wrapper:function(t){var e=t.children;return n.createElement(n.Fragment,{},e)}},k=n.forwardRef((function(t,e){var a=t.components,r=t.mdxType,l=t.originalType,d=t.parentName,s=o(t,["components","mdxType","originalType","parentName"]),u=p(a),k=r,g=u["".concat(d,".").concat(k)]||u[k]||m[k]||l;return a?n.createElement(g,i(i({ref:e},s),{},{components:a})):n.createElement(g,i({ref:e},s))}));function g(t,e){var a=arguments,r=e&&e.mdxType;if("string"==typeof t||r){var l=a.length,i=new Array(l);i[0]=k;var o={};for(var d in e)hasOwnProperty.call(e,d)&&(o[d]=e[d]);o.originalType=t,o[u]="string"==typeof t?t:r,i[1]=o;for(var p=2;p<l;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}k.displayName="MDXCreateElement"},2312:function(t,e,a){a.r(e),a.d(e,{assets:function(){return s},contentTitle:function(){return d},default:function(){return g},frontMatter:function(){return o},metadata:function(){return p},toc:function(){return u}});var n=a(7462),r=a(3366),l=(a(7294),a(3905)),i=["components"],o={id:"datastacks",title:"Datastacks overview",sidebar_label:"Datastacks",hide_title:!1,hide_table_of_contents:!1,description:"Overview of the Datastacks utility",keywords:["data","python","etl","cli","azure","template"]},d=void 0,p={unversionedId:"workloads/azure/data/etl_pipelines/datastacks",id:"workloads/azure/data/etl_pipelines/datastacks",title:"Datastacks overview",description:"Overview of the Datastacks utility",source:"@site/docs/workloads/azure/data/etl_pipelines/datastacks.md",sourceDirName:"workloads/azure/data/etl_pipelines",slug:"/workloads/azure/data/etl_pipelines/datastacks",permalink:"/docs/workloads/azure/data/etl_pipelines/datastacks",draft:!1,tags:[],version:"current",frontMatter:{id:"datastacks",title:"Datastacks overview",sidebar_label:"Datastacks",hide_title:!1,hide_table_of_contents:!1,description:"Overview of the Datastacks utility",keywords:["data","python","etl","cli","azure","template"]},sidebar:"docs",previous:{title:"Data Quality",permalink:"/docs/workloads/azure/data/etl_pipelines/data_quality_azure"},next:{title:"PySparkle",permalink:"/docs/workloads/azure/data/etl_pipelines/pysparkle"}},s={},u=[{value:"Using the Datastacks CLI",id:"using-the-datastacks-cli",level:2},{value:"Generating data workloads",id:"generating-data-workloads",level:2},{value:"Commands",id:"commands",level:3},{value:"Examples",id:"examples",level:3},{value:"Configuration",id:"configuration",level:3},{value:"Performing data quality checks",id:"performing-data-quality-checks",level:2},{value:"Required configuration",id:"required-configuration",level:3}],m={toc:u},k="wrapper";function g(t){var e=t.components,a=(0,r.Z)(t,i);return(0,l.kt)(k,(0,n.Z)({},m,a,{components:e,mdxType:"MDXLayout"}),(0,l.kt)("p",null,"Datastacks is a utility built to support various functions within the Ensono Stacks Data Platform. The library and its associated Python-based CLI is intended to assist developers working within a deployed Stacks Data Platform, supporting common tasks such as generating new data engineering workloads and running Spark jobs."),(0,l.kt)("h2",{id:"using-the-datastacks-cli"},"Using the Datastacks CLI"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/dev_quickstart_data_azure"},"Setup project environment"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"# Option 1: Run Datastacks CLI using Poetry's interactive shell (recommended for local development)\npoetry shell\ndatastacks --help\n\n# Option 2: Run Datastacks CLI using poetry run (recommended where Poetry shell cannot be used, e.g. CI/CD pipelines)\npoetry run datastacks --help\n")),(0,l.kt)("h2",{id:"generating-data-workloads"},"Generating data workloads"),(0,l.kt)("p",null,"Datastacks can be used to generate all the resources required for a new data engineering workload - for example a data ingest pipeline. This will create all resources required for the workload, based upon templates within the ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/ensono/stacks-azure-data/tree/main/de_templates"},"de_templates")," directory."),(0,l.kt)("p",null,"The ",(0,l.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/architecture/architecture_data_azure#data-engineering-workloads"},"deployment architecture")," section shows the workflow for using Datastacks to generate a new workload.\nSee ",(0,l.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/getting_started/etl_pipelines_deployment_azure"},"ETL Pipeline Deployment")," for a step-by-step guide on deploying a new workload using Datastacks."),(0,l.kt)("h3",{id:"commands"},"Commands"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"strong"},"generate")),": This command contains subcommands which generate components for the data platform given a config file.",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"strong"},"ingest")),": This subcommand utilises the template for ingest data pipelines, and uses a given config file to generate the required code for a new ingest pipeline ready for use. A flag can be included to specify whether to include data quality components in the pipeline.")))),(0,l.kt)("h3",{id:"examples"},"Examples"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'# Activate virtual environment\npoetry shell\n\n# Generate resources for an ingest pipeline\ndatastacks generate ingest --config="de_templates/test_config_ingest.yaml"\n\n# Generate resources for an ingest pipeline, with added Data Quality steps\ndatastacks generate ingest --config="de_templates/test_config_ingest.yaml" --data-quality\n')),(0,l.kt)("h3",{id:"configuration"},"Configuration"),(0,l.kt)("p",null,"In order to generate a new data engineering workload the Datastacks CLI takes a path to a config file. This config file should be YAML format, and contain configuration values as specified in the table below. A sample config file is included in the ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/ensono/stacks-azure-data/tree/main/de_templates"},"de_templates")," folder. The structure of config is ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/ensono/stacks-azure-data/tree/main/datastacks/datastacks/config.py"},"validated using Pydantic"),"."),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Config field"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"),(0,l.kt)("th",{parentName:"tr",align:null},"Required?"),(0,l.kt)("th",{parentName:"tr",align:null},"Format"),(0,l.kt)("th",{parentName:"tr",align:null},"Default value"),(0,l.kt)("th",{parentName:"tr",align:null},"Example value"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"dataset_name"),(0,l.kt)("td",{parentName:"tr",align:null},"Dataset name, used to derive pipeline and linked service names, e.g. AzureSql_Example."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"AzureSql_Demo")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"pipeline_description"),(0,l.kt)("td",{parentName:"tr",align:null},"Description of the pipeline to be created. Will be used for the Data Factory pipeline description."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},'"Ingest from demo Azure SQL database using ingest config file."')),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"data_source_type"),(0,l.kt)("td",{parentName:"tr",align:null},"Data source type."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"String",(0,l.kt)("br",null),(0,l.kt)("br",null),"Allowed values",(0,l.kt)("sup",{parentName:"td",id:"fnref-1"},(0,l.kt)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1")),":",(0,l.kt)("br",null),'"azure_sql"'),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"azure_sql")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"key_vault_linked_service_name"),(0,l.kt)("td",{parentName:"tr",align:null},"Name of the Key Vault linked service in Data Factory."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},"ls_KeyVault"),(0,l.kt)("td",{parentName:"tr",align:null},"ls_KeyVault")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"data_source_password_key_vault_secret_name"),(0,l.kt)("td",{parentName:"tr",align:null},"Secret name of the data source password in Key Vault."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"sql-password")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"data_source_connection_string_variable_name"),(0,l.kt)("td",{parentName:"tr",align:null},"Variable name for the connection string."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"sql_connection")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ado_variable_groups_nonprod"),(0,l.kt)("td",{parentName:"tr",align:null},"List of required variable groups in non-production environment."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"List","[String]"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"- amido-stacks-de-pipeline-nonprod",(0,l.kt)("br",null),"- stacks-credentials-nonprod-kv")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ado_variable_groups_prod"),(0,l.kt)("td",{parentName:"tr",align:null},"List of required variable groups in production environment."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"List","[String]"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"- amido-stacks-de-pipeline-prod",(0,l.kt)("br",null),"- stacks-credentials-prod-kv")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"default_arm_deployment_mode"),(0,l.kt)("td",{parentName:"tr",align:null},"Deployment mode for terraform."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},'"Incremental"'),(0,l.kt)("td",{parentName:"tr",align:null},"Incremental")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"window_start_default"),(0,l.kt)("td",{parentName:"tr",align:null},"Default window start date in the Data Factory pipeline."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"Date"),(0,l.kt)("td",{parentName:"tr",align:null},'"2010-01-01"'),(0,l.kt)("td",{parentName:"tr",align:null},"2010-01-01")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"window_end_default"),(0,l.kt)("td",{parentName:"tr",align:null},"Default window end date in the Data Factory pipeline."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"Date"),(0,l.kt)("td",{parentName:"tr",align:null},'"2010-01-31"'),(0,l.kt)("td",{parentName:"tr",align:null},"2010-01-31")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"bronze_container"),(0,l.kt)("td",{parentName:"tr",align:null},"Name of container for Bronze data"),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},"raw"),(0,l.kt)("td",{parentName:"tr",align:null},"raw")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"silver_container"),(0,l.kt)("td",{parentName:"tr",align:null},"Name of container for Silver data"),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},"staging"),(0,l.kt)("td",{parentName:"tr",align:null},"staging")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gold_container"),(0,l.kt)("td",{parentName:"tr",align:null},"Name of container for Gold data"),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},"curated"),(0,l.kt)("td",{parentName:"tr",align:null},"curated")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"trigger_start"),(0,l.kt)("td",{parentName:"tr",align:null},"Start datetime for Data Factory pipeline trigger."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"Datetime"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"2010-01-01T00:00:00Z")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"trigger_end"),(0,l.kt)("td",{parentName:"tr",align:null},"Datetime to set as end time for pipeline trigger."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"Datetime"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"2011-12-31T23:59:59Z")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"trigger_frequency"),(0,l.kt)("td",{parentName:"tr",align:null},"Frequency for the Data Factory pipeline trigger."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"String",(0,l.kt)("br",null),(0,l.kt)("br",null),"Allowed values:",(0,l.kt)("br",null),'"Minute"',(0,l.kt)("br",null),'"Hour"',(0,l.kt)("br",null),'"Day"',(0,l.kt)("br",null),'"Week"',(0,l.kt)("br",null),'"Month"'),(0,l.kt)("td",{parentName:"tr",align:null},'"Month"'),(0,l.kt)("td",{parentName:"tr",align:null},"Month")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"trigger_interval"),(0,l.kt)("td",{parentName:"tr",align:null},"Interval value for the Data Factory pipeline trigger."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"Integer"),(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"1")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"trigger_delay"),(0,l.kt)("td",{parentName:"tr",align:null},"Delay between Data Factory pipeline triggers, formatted HH:mm:ss"),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},'"02:00:00"'),(0,l.kt)("td",{parentName:"tr",align:null},"02:00:00")))),(0,l.kt)("h2",{id:"performing-data-quality-checks"},"Performing data quality checks"),(0,l.kt)("p",null,"Datastacks provides a CLI to conduct data quality checks using the ",(0,l.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/etl_pipelines/pysparkle"},"PySparkle")," library as a backend."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'datastacks dq --help\ndatastacks dq --config-path "ingest/Ingest_AzureSql_Example/data_quality/ingest_dq.json" --container config\n')),(0,l.kt)("h3",{id:"required-configuration"},"Required configuration"),(0,l.kt)("p",null,"For details regarding the required environment settings and the configuration file please read the ",(0,l.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/etl_pipelines/data_quality_azure#usage"},"Data Quality")," section."),(0,l.kt)("div",{className:"footnotes"},(0,l.kt)("hr",{parentName:"div"}),(0,l.kt)("ol",{parentName:"div"},(0,l.kt)("li",{parentName:"ol",id:"fn-1"},"Additional data source types will be supported in future.",(0,l.kt)("a",{parentName:"li",href:"#fnref-1",className:"footnote-backref"},"\u21a9")))))}g.isMDXComponent=!0}}]);