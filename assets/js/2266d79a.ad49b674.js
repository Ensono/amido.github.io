"use strict";(self.webpackChunkstacks=self.webpackChunkstacks||[]).push([[8519],{3905:function(e,t,a){a.d(t,{Zo:function(){return u},kt:function(){return k}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),d=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},u=function(e){var t=d(e.components);return n.createElement(s.Provider,{value:t},e.children)},p="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),p=d(a),m=r,k=p["".concat(s,".").concat(m)]||p[m]||c[m]||i;return a?n.createElement(k,l(l({ref:t},u),{},{components:a})):n.createElement(k,l({ref:t},u))}));function k(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,l=new Array(i);l[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[p]="string"==typeof e?e:r,l[1]=o;for(var d=2;d<i;d++)l[d]=a[d];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},2510:function(e,t,a){a.r(t),a.d(t,{assets:function(){return u},contentTitle:function(){return s},default:function(){return k},frontMatter:function(){return o},metadata:function(){return d},toc:function(){return p}});var n=a(7462),r=a(3366),i=(a(7294),a(3905)),l=["components"],o={id:"stacks_data_utilities",title:"Stacks Data Utilities",sidebar_label:"Stacks Data Utilities",hide_title:!1,hide_table_of_contents:!1,description:"Stacks Data utilities overview",keywords:["pyspark","spark","python","etl"]},s=void 0,d={unversionedId:"workloads/azure/data/data_engineering/stacks_data_utilities",id:"workloads/azure/data/data_engineering/stacks_data_utilities",title:"Stacks Data Utilities",description:"Stacks Data utilities overview",source:"@site/docs/workloads/azure/data/data_engineering/stacks_data_utilities.md",sourceDirName:"workloads/azure/data/data_engineering",slug:"/workloads/azure/data/data_engineering/stacks_data_utilities",permalink:"/docs/workloads/azure/data/data_engineering/stacks_data_utilities",draft:!1,tags:[],version:"current",frontMatter:{id:"stacks_data_utilities",title:"Stacks Data Utilities",sidebar_label:"Stacks Data Utilities",hide_title:!1,hide_table_of_contents:!1,description:"Stacks Data utilities overview",keywords:["pyspark","spark","python","etl"]},sidebar:"docs",previous:{title:"Data Engineering Overview",permalink:"/docs/workloads/azure/data/data_engineering/data_engineering_intro_azure"},next:{title:"Datastacks CLI",permalink:"/docs/workloads/azure/data/data_engineering/datastacks"}},u={},p=[{value:"Setup",id:"setup",level:2},{value:"Azure environment variables",id:"azure-environment-variables",level:2},{value:"Storage account names",id:"storage-account-names",level:3},{value:"Running Spark jobs",id:"running-spark-jobs",level:3},{value:"Running end-to-end tests",id:"running-end-to-end-tests",level:3}],c={toc:p},m="wrapper";function k(e){var t=e.components,a=(0,r.Z)(e,l);return(0,i.kt)(m,(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://pypi.org/project/stacks-data/"},(0,i.kt)("strong",{parentName:"a"},"stacks-data"))," is a Python library, containing a suite of utilities to accelerate development within an Ensono Stacks Data Platform. It is an integral part of the platform, supporting common tasks such as generating new data engineering workloads and running Spark jobs. ",(0,i.kt)("strong",{parentName:"p"},"stacks-data")," consists of:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/data_engineering/datastacks"},"Datastacks CLI")," - A command-line interface for data engineers, enabling interaction with Datastacks' various functions."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/data_engineering/datastacks#generating-data-workloads"},"Data workload generation")," - Generate new data workloads based upon common templates."),(0,i.kt)("li",{parentName:"ul"},"PySpark utilities - A suite of reusable utilities to simplify development of data pipelines using Apache Spark and Python."),(0,i.kt)("li",{parentName:"ul"},"Data quality utilities - Utilities to support the ",(0,i.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/data_engineering/data_quality_azure"},"data quality framework")," implemented in Stacks."),(0,i.kt)("li",{parentName:"ul"},"Azure utilities - Utilities to support common interactions with Azure resources from data workloads."),(0,i.kt)("li",{parentName:"ul"},"Behave utilities - Common scenarios and setup used by ",(0,i.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/data_engineering/testing_data_azure#end-to-end-tests"},"Behave end-to-end tests"),".")),(0,i.kt)("h2",{id:"setup"},"Setup"),(0,i.kt)("p",null,"The following setup steps will ensure your development environment is setup correctly and install ",(0,i.kt)("strong",{parentName:"p"},"stacks-data")," in your Python virtual environment:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Install the ",(0,i.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/requirements_data_azure#local-development"},"local development requirements"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/dev_quickstart_data_azure"},"Setup your local development environment"),".")),(0,i.kt)("p",null,"Alternatively, you can directly install stacks-data from ",(0,i.kt)("a",{parentName:"p",href:"https://pypi.org/project/stacks-data/"},"PyPi"),", using:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install stacks-data\n")),(0,i.kt)("p",null,"For information on utilising ",(0,i.kt)("strong",{parentName:"p"},"stacks-data")," from within Databricks - see ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/getting_started/dev_quickstart_data_azure#optional-pyspark-development-in-databricks"},"development in Databricks"),"."),(0,i.kt)("h2",{id:"azure-environment-variables"},"Azure environment variables"),(0,i.kt)("p",null,"Several environment variables are required by stacks-data to interact with Azure services. The environment variables you require differ depending on which processes you are running, and where you are running them from (e.g. your local machine, a Databricks cluster, or a CICD pipeline). A Stacks Data Platform will automatically ensure the required environment variables are made available from CICD pipelines and Databricks job clusters. However, to run processes from your local machine or a different Databricks cluster, you will need to configure these manually."),(0,i.kt)("h3",{id:"storage-account-names"},"Storage account names"),(0,i.kt)("p",null,"Environment variables defining the storage account names are required both for running Spark jobs and triggering end-to-end tests - so should be defined wherever you run these tasks from (e.g. local machine, Databricks cluster)."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Environment variable name"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"),(0,i.kt)("th",{parentName:"tr",align:null},"Example value"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"ADLS_ACCOUNT")),(0,i.kt)("td",{parentName:"tr",align:null},"Azure Data Lake storage account name."),(0,i.kt)("td",{parentName:"tr",align:null},"amidostacksdeveuwdeadls")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"CONFIG_BLOB_ACCOUNT")),(0,i.kt)("td",{parentName:"tr",align:null},"Blob Storage account name used for config data."),(0,i.kt)("td",{parentName:"tr",align:null},"amidostacksdeveuwdeconfi")))),(0,i.kt)("h3",{id:"running-spark-jobs"},"Running Spark jobs"),(0,i.kt)("p",null,"For running Spark jobs, you need to define the storage account names as well as the environment variables below. If developing Spark jobs from within Databricks, these variables will need to be set on your cluster and should reference values from the key vault - see ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/getting_started/dev_quickstart_data_azure#optional-pyspark-development-in-databricks"},"PySpark development in Databricks"),"."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Environment variable name"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"),(0,i.kt)("th",{parentName:"tr",align:null},"Example value"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"AZURE_TENANT_ID")),(0,i.kt)("td",{parentName:"tr",align:null},"Directory ID for Azure Active Directory application."),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("em",{parentName:"td"},"00000000-0000-0000-0000-000000000000"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"AZURE_CLIENT_ID")),(0,i.kt)("td",{parentName:"tr",align:null},"Service Principal Application ID."),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("em",{parentName:"td"},"00000000-0000-0000-0000-000000000000"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"AZURE_CLIENT_SECRET")),(0,i.kt)("td",{parentName:"tr",align:null},"Service Principal Secret."),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("em",{parentName:"td"},"secretValue123456"))))),(0,i.kt)("h3",{id:"running-end-to-end-tests"},"Running end-to-end tests"),(0,i.kt)("p",null,"In order to trigger ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/testing_data_azure#end-to-end-tests"},"end-to-end tests"),", you need to define the storage account names as well as the environment variables below. Running end-to-end tests requires access to various Azure resources, for example to prepare and tidy up test data, and trigger Data Factory pipelines. ",(0,i.kt)("inlineCode",{parentName:"p"},"AZURE_TENANT_ID"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"AZURE_CLIENT_ID")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"AZURE_CLIENT_SECRET")," may also be provided to authenticate with Azure, or alternatively if running the tests locally you can authenticate by ",(0,i.kt)("a",{parentName:"p",href:"https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli"},"signing in to the Azure CLI"),"."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Environment variable name"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"),(0,i.kt)("th",{parentName:"tr",align:null},"Example value"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"AZURE_SUBSCRIPTION_ID")),(0,i.kt)("td",{parentName:"tr",align:null},"Azure subscription ID."),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("em",{parentName:"td"},"00000000-0000-0000-0000-000000000000"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"AZURE_RESOURCE_GROUP_NAME")),(0,i.kt)("td",{parentName:"tr",align:null},"Name of the resource group."),(0,i.kt)("td",{parentName:"tr",align:null},"amido-stacks-dev-euw-de")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"AZURE_DATA_FACTORY_NAME")),(0,i.kt)("td",{parentName:"tr",align:null},"Name of the Data Factory resource."),(0,i.kt)("td",{parentName:"tr",align:null},"amido-stacks-dev-euw-de")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"AZURE_REGION_NAME")),(0,i.kt)("td",{parentName:"tr",align:null},"Azure region in which the platform is deployed."),(0,i.kt)("td",{parentName:"tr",align:null},"West Europe")))))}k.isMDXComponent=!0}}]);