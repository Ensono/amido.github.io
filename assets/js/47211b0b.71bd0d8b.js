"use strict";(self.webpackChunkstacks=self.webpackChunkstacks||[]).push([[6899],{1924:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>n,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>l});var s=t(4848),i=t(8453);const r={id:"data_processing",title:"Data Processing Workloads",sidebar_label:"Data Processing Workloads",hide_title:!1,hide_table_of_contents:!1,description:"Data Processing Workloads",keywords:["silver","adf","etl","databricks"]},n=void 0,o={id:"workloads/azure/data/data_engineering/data_processing",title:"Data Processing Workloads",description:"Data Processing Workloads",source:"@site/docs/workloads/azure/data/data_engineering/data_processing.md",sourceDirName:"workloads/azure/data/data_engineering",slug:"/workloads/azure/data/data_engineering/data_processing",permalink:"/docs/workloads/azure/data/data_engineering/data_processing",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"data_processing",title:"Data Processing Workloads",sidebar_label:"Data Processing Workloads",hide_title:!1,hide_table_of_contents:!1,description:"Data Processing Workloads",keywords:["silver","adf","etl","databricks"]},sidebar:"docs",previous:{title:"Data Ingest Workloads",permalink:"/docs/workloads/azure/data/data_engineering/ingest_data_azure"},next:{title:"Data Quality",permalink:"/docs/workloads/azure/data/data_engineering/data_quality_azure"}},d={},l=[{value:"Data processing pipeline overview",id:"data-processing-pipeline-overview",level:2},{value:"Data Factory pipeline design",id:"data-factory-pipeline-design",level:3},{value:"Passing parameters from Data Factory",id:"passing-parameters-from-data-factory",level:3}];function c(e){const a={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.p,{children:"Data processing workloads in Ensono Stacks are jobs which:"}),"\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsx)(a.li,{children:"Take data in the data lake as input (this can be various formats e.g. CSV, Parquet, JSON, Delta)."}),"\n",(0,s.jsx)(a.li,{children:"Perform some form of data transformation / cleansing / modelling / aggregation over the data."}),"\n",(0,s.jsxs)(a.li,{children:["Output the into the data lake into a structured ",(0,s.jsx)(a.a,{href:"https://delta.io/",children:"Delta Lake"})," format."]}),"\n"]}),"\n",(0,s.jsxs)(a.p,{children:["Data processing workloads are based upon running Apache Spark / Python jobs on Databricks. These workloads may be used for various levels of data transformation and preparation within the data lake. Within the ",(0,s.jsx)(a.a,{href:"/docs/workloads/azure/data/data_engineering/data_engineering_intro_azure#medallion-architecture",children:"medallion architecture"})," these will include:"]}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:"Bronze to Silver"}),"\n",(0,s.jsx)(a.li,{children:"Silver to Gold"}),"\n"]}),"\n",(0,s.jsxs)(a.p,{children:["Processing workloads will utilise the ",(0,s.jsx)(a.a,{href:"/docs/workloads/azure/data/data_engineering/stacks_data_utilities",children:"Stacks Data Python library"})," to support and standardise common tasks. Similar to ingest workloads, data processing workloads can also optionally include a ",(0,s.jsx)(a.a,{href:"/docs/workloads/azure/data/data_engineering/data_quality_azure",children:"data quality validations"})," step."]}),"\n",(0,s.jsx)(a.p,{children:"The following example data processing workloads are included for reference:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.a,{href:"https://github.com/Ensono/stacks-azure-data/tree/main/de_workloads/data_processing/silver_movies_example",children:"silver_movies_example"}),": Performs Bronze to Silver layer data processing over the ",(0,s.jsx)(a.a,{href:"/docs/workloads/azure/data/getting_started/example_data_source",children:"example dataset"}),"."]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.a,{href:"https://github.com/Ensono/stacks-azure-data/tree/main/de_workloads/data_processing/gold_movies_example",children:"gold_movies_example"}),": Aggregates and joins data from the Silver layer example above to create a Gold layer dataset."]}),"\n"]}),"\n",(0,s.jsx)(a.h2,{id:"data-processing-pipeline-overview",children:"Data processing pipeline overview"}),"\n",(0,s.jsxs)(a.p,{children:["Within Stacks, processing activities are performed using Python PySpark jobs. These jobs are orchestrated by pipelines in Data Factory, and executed in Databricks. Using PySpark jobs - as opposed to notebooks - gives full control over the processing activities (for example ensuring thorough ",(0,s.jsx)(a.a,{href:"/docs/workloads/azure/data/data_engineering/testing_data_azure",children:"test coverage"})," and quality control)."]}),"\n",(0,s.jsx)(a.p,{children:"The diagram below gives an example of a data processing data pipeline in Data Factory."}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.img,{alt:"ADF_SilverGoldPipelineDesign.png",src:t(9702).A+"",width:"2775",height:"1151"})}),"\n",(0,s.jsxs)(a.p,{children:["The Python PySpark script executed is defined in ",(0,s.jsx)(a.code,{children:"spark_jobs/process.py"})," within the workload's directory. This job will utilise the ",(0,s.jsx)(a.a,{href:"/docs/workloads/azure/data/data_engineering/stacks_data_utilities",children:"Stacks Data library"}),", which provides a wealth of reusable utilities to assist with data transformations and loading data from/into to the data lake. The script gets uploaded to DBFS (",(0,s.jsx)(a.code,{children:"dbfs:/FileStore/scripts/pipeline_name/process.py"}),") as part of the deployment pipeline, so it is accessible to Databricks."]}),"\n",(0,s.jsx)(a.h3,{id:"data-factory-pipeline-design",children:"Data Factory pipeline design"}),"\n",(0,s.jsxs)(a.p,{children:["Within Data Factory, the processing pipelines are kept within the ",(0,s.jsx)(a.code,{children:"Process"})," folder:"]}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.img,{alt:"ADF_SilverPipelinesList.png",src:t(779).A+"",width:"283",height:"243"})}),"\n",(0,s.jsx)(a.p,{children:"An example pipeline is shown below:"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.img,{alt:"ADF_Silver.png",src:t(6156).A+"",width:"1032",height:"776"})}),"\n",(0,s.jsxs)(a.p,{children:["The pipeline contains a Python Databricks activity which runs the ",(0,s.jsx)(a.code,{children:"process.py"})," file. The ",(0,s.jsx)(a.a,{href:"/docs/workloads/azure/data/data_engineering/stacks_data_utilities",children:"stacks-data library"})," is installed on the cluster via PyPi."]}),"\n",(0,s.jsx)(a.admonition,{type:"tip",children:(0,s.jsxs)(a.p,{children:["It is recommended that specific versions of packages are defined in the Databricks Library configuration (e.g. ",(0,s.jsx)(a.code,{children:"stacks-data==0.1.2"}),") to ensure a reproducible environment for running jobs. This is the default approach when generating workloads using ",(0,s.jsx)(a.a,{href:"/docs/workloads/azure/data/data_engineering/datastacks",children:"Datastacks CLI"}),". If the version is omitted (e.g. ",(0,s.jsx)(a.code,{children:"stacks-data"}),"), then Databricks will use the latest available version - however this is not recommended."]})}),"\n",(0,s.jsx)(a.h3,{id:"passing-parameters-from-data-factory",children:"Passing parameters from Data Factory"}),"\n",(0,s.jsx)(a.p,{children:"You may pass parameters from Data Factory to the Python job executed in Databricks. These are defined in the Settings section of the Databricks Python activity:"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.img,{alt:"adf-databricks-parameters.png",src:t(3510).A+"",width:"735",height:"476"})}),"\n",(0,s.jsxs)(a.p,{children:["These parameters can then be accessed from the Python file, by using the ",(0,s.jsx)(a.code,{children:"get_data_factory_param"})," function - for example the below could be used to access the first parameter shown above:"]}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'from stacks.data.pyspark.etl import get_data_factory_param\n\nrun_id = get_data_factory_param(1, "default_run_id")\n'})})]})}function p(e={}){const{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},9702:(e,a,t)=>{t.d(a,{A:()=>s});const s=t.p+"assets/images/ADF_SilverGoldPipelineDesign-45f67d23a527feb7cca909593202ce35.png"},779:(e,a,t)=>{t.d(a,{A:()=>s});const s=t.p+"assets/images/ADF_SilverPipelinesList-ad113e315740f36f95fedaf0df540032.png"},6156:(e,a,t)=>{t.d(a,{A:()=>s});const s=t.p+"assets/images/ADF_silver-3d7a3a5b4c6dd85eb502a3c33d83bd9c.png"},3510:(e,a,t)=>{t.d(a,{A:()=>s});const s=t.p+"assets/images/adf-databricks-parameters-1c7ac7791ea4c2180e0d293c9d20ed46.png"},8453:(e,a,t)=>{t.d(a,{R:()=>n,x:()=>o});var s=t(6540);const i={},r=s.createContext(i);function n(e){const a=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:n(e.components),s.createElement(r.Provider,{value:a},e.children)}}}]);