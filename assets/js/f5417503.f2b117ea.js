"use strict";(self.webpackChunkstacks=self.webpackChunkstacks||[]).push([[5602],{848:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/ADF_Ingest_AzureSql_Example_DQ-0984cf1a527965a42d99441f40ca98fd.png"},4198:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/ADF_DataQualityDesign-dd10d4958d37b1136716ef1d27c0002e.png"},4233:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>d,metadata:()=>n,toc:()=>o});const n=JSON.parse('{"id":"workloads/azure/data/data_engineering/data_quality_azure","title":"Data Quality","description":"Approach to DQ validations","source":"@site/docs/workloads/azure/data/data_engineering/data_quality_azure.md","sourceDirName":"workloads/azure/data/data_engineering","slug":"/workloads/azure/data/data_engineering/data_quality_azure","permalink":"/docs/workloads/azure/data/data_engineering/data_quality_azure","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"data_quality_azure","title":"Data Quality","sidebar_label":"Data Quality","hide_title":false,"hide_table_of_contents":false,"description":"Approach to DQ validations","keywords":["data quality","great expectations"]},"sidebar":"docs","previous":{"title":"Data Processing Workloads","permalink":"/docs/workloads/azure/data/data_engineering/data_processing"},"next":{"title":"Testing","permalink":"/docs/workloads/azure/data/data_engineering/testing_data_azure"}}');var i=t(4848),s=t(8453);const d={id:"data_quality_azure",title:"Data Quality",sidebar_label:"Data Quality",hide_title:!1,hide_table_of_contents:!1,description:"Approach to DQ validations",keywords:["data quality","great expectations"]},r=void 0,l={},o=[{value:"Data pipelines with quality checks",id:"data-pipelines-with-quality-checks",level:2},{value:"Ingest pipeline example",id:"ingest-pipeline-example",level:3},{value:"Processing pipeline example",id:"processing-pipeline-example",level:3},{value:"Interactive usage",id:"interactive-usage",level:2},{value:"JSON Configuration File for Great Expectations",id:"json-configuration-file-for-great-expectations",level:2},{value:"Using environment variables in configuration files",id:"using-environment-variables-in-configuration-files",level:3},{value:"Example",id:"example",level:3},{value:"Validation results",id:"validation-results",level:2}];function c(e){const a={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(a.p,{children:"Ensono Stacks data workloads can be deployed with additional data quality checks. These checks validate that the outputs of\na data pipeline meet specified requirements, expressed in a simple, human-readable language. It allows\nyou to assert expectations about your data, which can help catch any discrepancies, anomalies, or\nerrors in your data as early in the pipeline as possible."}),"\n",(0,i.jsxs)(a.p,{children:["Data quality checks are executed as Python Databricks jobs with the quality validation logic\npackaged within the ",(0,i.jsx)(a.a,{href:"/docs/workloads/azure/data/data_engineering/stacks_data_utilities",children:"Stacks Data library"}),".\nInternally, this leverages the capabilities of the ",(0,i.jsx)(a.a,{href:"https://greatexpectations.io/",children:"Great Expectations"}),"\nlibrary, an open-source Python library, to perform these checks."]}),"\n",(0,i.jsx)(a.p,{children:"The design of the data quality processing is outlined in the following diagram."}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.img,{alt:"ADF_Ingest_AzureSql_Example_DQ.png",src:t(4198).A+"",width:"3337",height:"1084"})}),"\n",(0,i.jsx)(a.h2,{id:"data-pipelines-with-quality-checks",children:"Data pipelines with quality checks"}),"\n",(0,i.jsxs)(a.p,{children:["Both ",(0,i.jsx)(a.a,{href:"/docs/workloads/azure/data/data_engineering/ingest_data_azure",children:"data ingest"})," and ",(0,i.jsx)(a.a,{href:"/docs/workloads/azure/data/data_engineering/data_processing",children:"data processing"})," pipelines may be generated with a data quality step - see ",(0,i.jsx)(a.a,{href:"/docs/workloads/azure/data/data_engineering/datastacks#generating-data-workloads",children:"generating data workloads"})," for details."]}),"\n",(0,i.jsxs)(a.p,{children:["The generated workloads will contain a Python script within ",(0,i.jsx)(a.code,{children:"spark_jobs/data_quality.py"}),". This script will be executed via a Databricks Python activity in Data Factory. The script gets uploaded to DBFS (",(0,i.jsx)(a.code,{children:"dbfs:/FileStore/scripts/pipeline_name/data_quality.py"}),") as part of the deployment pipeline, so it is accessible to Databricks."]}),"\n",(0,i.jsx)(a.h3,{id:"ingest-pipeline-example",children:"Ingest pipeline example"}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.img,{alt:"ADF_Ingest_AzureSql_Example_DQ.png",src:t(848).A+"",width:"1046",height:"285"})}),"\n",(0,i.jsx)(a.h3,{id:"processing-pipeline-example",children:"Processing pipeline example"}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.img,{alt:"ADF_silver_dq.png",src:t(5228).A+"",width:"667",height:"162"})}),"\n",(0,i.jsx)(a.h2,{id:"interactive-usage",children:"Interactive usage"}),"\n",(0,i.jsxs)(a.p,{children:["You can use the ",(0,i.jsx)(a.a,{href:"/docs/workloads/azure/data/data_engineering/datastacks",children:"Datastacks CLI"})," to perform data quality checks against a workload interactively. Note, this requires that the ",(0,i.jsx)(a.a,{href:"/docs/workloads/azure/data/data_engineering/stacks_data_utilities#azure-environment-variables",children:"Azure environment variables"})," are set:"]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-bash",children:'datastacks dq --help\n\n# Execute data quality checks using the provided config\ndatastacks dq --config-path "ingest/ingest_azure_sql_example/data_quality/ingest_dq.json" --container config\n'})}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.code,{children:"config-path"})," is a path to a JSON config inside the Azure Blob container, ",(0,i.jsx)(a.code,{children:"container"})," is name of the container. The storage account used will be based upon the ",(0,i.jsx)(a.code,{children:"CONFIG_BLOB_ACCOUNT"})," environment variable."]}),"\n",(0,i.jsx)(a.h2,{id:"json-configuration-file-for-great-expectations",children:"JSON Configuration File for Great Expectations"}),"\n",(0,i.jsx)(a.p,{children:"This section describes the structure of the JSON configuration file used for the data quality process.\nThe configuration is defined using Python's Pydantic library for data validation."}),"\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"gx_directory_path"}),": Path to the Great Expectations metadata store."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"dataset_name"}),": Name of the dataset that is being processed."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"dq_input_path"}),": Path to where the input data is stored. For data stored in files, this would be a URI, e.g. ",(0,i.jsx)(a.code,{children:'"abfss://raw@accountname.dfs.core.windows.net/ingest_azure_sql_example/"'})]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"dq_output_path"}),": Path to where data quality results will be written."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"datasource_config"}),": Configuration for the dataset being tested.","\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"datasource_name"}),": Name of the data asset, e.g. table or file name."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"datasource_type"}),": Data type of the asset - this can be any type that Spark can read from, e.g. table, parquet, csv, delta."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"data_location"}),": Location of the given data asset.","\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:["If the data is stored in files, like a Parquet file on ADLS, you should\nprovide the complete path to the file within the ",(0,i.jsx)(a.code,{children:"dq_input_path"}),". Examples:","\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:(0,i.jsx)(a.code,{children:'"myfolder/mysubfolder/myfile.parquet"'})}),"\n",(0,i.jsx)(a.li,{children:(0,i.jsx)(a.code,{children:'"myfolder/mysubfolder/*"'})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(a.li,{children:["For tables with metadata managed by a data catalog, you should provide\nthe database schema and the table name. Example:","\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:(0,i.jsx)(a.code,{children:"staging.table_name"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"expectation_suite_name"}),": Name of the expectation suite associated with this data source."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"validation_config"}),": A list of validation configurations where each configuration contains the following fields:","\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"column_name"}),": Name of the validated column."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"expectations"}),": List of expectations where each expectation has the following fields:","\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"expectation_type"}),": Name of the Great Expectations ",(0,i.jsx)(a.a,{href:"https://greatexpectations.io/expectations/",children:"expectation class"})," to use."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"expectation_kwargs"}),": The keyword arguments to pass to the expectation class."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"using-environment-variables-in-configuration-files",children:"Using environment variables in configuration files"}),"\n",(0,i.jsxs)(a.p,{children:["It is possible to use environment variables in a configuration file for data quality.\nPlaceholders in the form of ",(0,i.jsx)(a.code,{children:"{ENV_VAR_NAME}"})," will be replaced with the corresponding environment\nvariable values. For example, you can pass the ADLS name using an environment variable:"]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-json",children:'{\n  "dq_input_path": "abfss://raw@{ADLS_ACCOUNT}.dfs.core.windows.net/ingest_azure_sql_example/"\n}\n'})}),"\n",(0,i.jsx)(a.h3,{id:"example",children:"Example"}),"\n",(0,i.jsx)(a.p,{children:"Here's a minimal example of a configuration file:"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-json",children:'{\n    "gx_directory_path": "/dbfs/great_expectations/",\n    "dataset_name": "movies",\n    "dq_input_path": "abfss://raw@{ADLS_ACCOUNT}.dfs.core.windows.net/ingest_azure_sql_example/",\n    "dq_output_path": "abfss://raw@{ADLS_ACCOUNT}.dfs.core.windows.net/ingest_azure_sql_example/",\n    "datasource_config": [\n        {\n            "datasource_name": "movies.movies_metadata",\n            "datasource_type": "parquet",\n            "data_location": "movies.movies_metadata/v1/*/*/*",\n            "expectation_suite_name": "movies.movies_metadata_suite",\n            "validation_config": [\n                {\n                    "column_name": "status",\n                    "expectations": [\n                        {\n                            "expectation_type": "expect_column_values_to_be_in_set",\n                            "expectation_kwargs": {"value_set": ["Canceled", "In Production", "Planned", "Post Production", "Released", "Rumored"]}\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n'})}),"\n",(0,i.jsx)(a.h2,{id:"validation-results",children:"Validation results"}),"\n",(0,i.jsxs)(a.p,{children:["Results of the data quality checks are stored in the ",(0,i.jsx)(a.code,{children:"dq_output_path"})," location in Delta tables. Their names follow the format ",(0,i.jsx)(a.code,{children:"{datasource_name}_dq"}),", e.g. ",(0,i.jsx)(a.code,{children:"movies.movies_metadata_dq"}),". The results contain the following columns:"]}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{children:"Column Name"}),(0,i.jsx)(a.th,{children:"Type"}),(0,i.jsx)(a.th,{children:"Description"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"data_quality_run_date"}),(0,i.jsx)(a.td,{children:"date"}),(0,i.jsx)(a.td,{children:"Timestamp of the data quality run."})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"datasource_name"}),(0,i.jsx)(a.td,{children:"string"}),(0,i.jsx)(a.td,{children:"Name of the data asset."})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"column_name"}),(0,i.jsx)(a.td,{children:"string"}),(0,i.jsx)(a.td,{children:"Name of the column."})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"validator"}),(0,i.jsx)(a.td,{children:"string"}),(0,i.jsx)(a.td,{children:"Name of the validator."})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"value_set"}),(0,i.jsx)(a.td,{children:"string"}),(0,i.jsx)(a.td,{children:"The set of values a column should have (if applicable)."})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"threshold"}),(0,i.jsx)(a.td,{children:"string"}),(0,i.jsx)(a.td,{children:"The percentage of rows that should pass the validation for the check to be considered successful."})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"failure_count"}),(0,i.jsx)(a.td,{children:"string"}),(0,i.jsx)(a.td,{children:"The number of rows that failed the validation."})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"failure_percent"}),(0,i.jsx)(a.td,{children:"string"}),(0,i.jsx)(a.td,{children:"The percentage of rows that failed the validation (expressed as a decimal between 0 and 1)."})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"failure_query"}),(0,i.jsx)(a.td,{children:"string"}),(0,i.jsx)(a.td,{children:"A query that can be used to retrieve the rows that failed the validation."})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"dq_check_exception"}),(0,i.jsx)(a.td,{children:"boolean"}),(0,i.jsx)(a.td,{children:"True if an exception occurred while running the check."})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"exception_message"}),(0,i.jsx)(a.td,{children:"string"}),(0,i.jsx)(a.td,{children:"The exception message if an exception was raised."})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{children:"success"}),(0,i.jsx)(a.td,{children:"boolean"}),(0,i.jsx)(a.td,{children:"True if the dataset passed the check."})]})]})]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,i.jsx)(a,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},5228:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/ADF_silver_dq-d96cf8d92fecbad74202efecb01f5510.png"},8453:(e,a,t)=>{t.d(a,{R:()=>d,x:()=>r});var n=t(6540);const i={},s=n.createContext(i);function d(e){const a=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function r(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:d(e.components),n.createElement(s.Provider,{value:a},e.children)}}}]);