"use strict";(self.webpackChunkstacks=self.webpackChunkstacks||[]).push([[230],{3905:function(e,t,a){a.d(t,{Zo:function(){return p},kt:function(){return g}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),d=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=d(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=d(a),m=r,g=u["".concat(l,".").concat(m)]||u[m]||c[m]||o;return a?n.createElement(g,i(i({ref:t},p),{},{components:a})):n.createElement(g,i({ref:t},p))}));function g(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:r,i[1]=s;for(var d=2;d<o;d++)i[d]=a[d];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},4264:function(e,t,a){a.r(t),a.d(t,{assets:function(){return p},contentTitle:function(){return l},default:function(){return g},frontMatter:function(){return s},metadata:function(){return d},toc:function(){return u}});var n=a(7462),r=a(3366),o=(a(7294),a(3905)),i=["components"],s={id:"ingest_pipeline_deployment_azure",title:"Data Ingest Pipeline Deployment",sidebar_label:"6. Data Ingest Pipeline Deployment",hide_title:!1,hide_table_of_contents:!1,description:"Data ingest pipelines development & deployment",keywords:["datastacks","data","python","etl","cli","azure","template"]},l=void 0,d={unversionedId:"workloads/azure/data/getting_started/ingest_pipeline_deployment_azure",id:"workloads/azure/data/getting_started/ingest_pipeline_deployment_azure",title:"Data Ingest Pipeline Deployment",description:"Data ingest pipelines development & deployment",source:"@site/docs/workloads/azure/data/getting_started/ingest_pipeline_deployment_azure.md",sourceDirName:"workloads/azure/data/getting_started",slug:"/workloads/azure/data/getting_started/ingest_pipeline_deployment_azure",permalink:"/docs/workloads/azure/data/getting_started/ingest_pipeline_deployment_azure",draft:!1,tags:[],version:"current",frontMatter:{id:"ingest_pipeline_deployment_azure",title:"Data Ingest Pipeline Deployment",sidebar_label:"6. Data Ingest Pipeline Deployment",hide_title:!1,hide_table_of_contents:!1,description:"Data ingest pipelines development & deployment",keywords:["datastacks","data","python","etl","cli","azure","template"]},sidebar:"docs",previous:{title:"5. Example Data Source",permalink:"/docs/workloads/azure/data/getting_started/example_data_source"},next:{title:"7. Data Processing Pipeline Deployment",permalink:"/docs/workloads/azure/data/getting_started/processing_pipeline_deployment_azure"}},p={},u=[{value:"Data source pre-requisites",id:"data-source-pre-requisites",level:2},{value:"Azure DevOps variable",id:"azure-devops-variable",level:3},{value:"Key Vault secret",id:"key-vault-secret",level:3},{value:"Step 1: Create feature branch",id:"step-1-create-feature-branch",level:2},{value:"Step 2: Prepare the Datastacks config file",id:"step-2-prepare-the-datastacks-config-file",level:2},{value:"Step 3: Generate project artifacts using Datastacks",id:"step-3-generate-project-artifacts-using-datastacks",level:2},{value:"Step 4: Update ingest configuration",id:"step-4-update-ingest-configuration",level:2},{value:"Step 5: Update end-to-end tests",id:"step-5-update-end-to-end-tests",level:2},{value:"Step 6: Deploy new workload in non-production environment",id:"step-6-deploy-new-workload-in-non-production-environment",level:2},{value:"Step 7: Review deployed resources",id:"step-7-review-deployed-resources",level:2},{value:"Step 8: Deploy new workload in further environments",id:"step-8-deploy-new-workload-in-further-environments",level:2},{value:"Next steps",id:"next-steps",level:2}],c={toc:u},m="wrapper";function g(e){var t=e.components,a=(0,r.Z)(e,i);return(0,o.kt)(m,(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"This section provides an overview of generating a new ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/ingest_data_azure"},"data ingest pipeline")," workload and deploying it into a Ensono Stacks Data Platform, using the ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/datastacks"},"Datastacks CLI"),"."),(0,o.kt)("p",null,"This guide assumes the following are in place:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A ",(0,o.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/core_data_platform_deployment_azure"},"deployed Ensono Stacks Data Platform")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/dev_quickstart_data_azure"},"Development environment set up")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/shared_resources_deployment_azure"},"Deployed shared resources")),(0,o.kt)("li",{parentName:"ul"},"A data source to ingest from. The steps below are based on using the ",(0,o.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/example_data_source"},"Azure SQL example data source"))),(0,o.kt)("p",null,"This process will deploy the following resources into the project:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Azure Data Factory resources (defined in Terraform / ARM)",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Linked service"),(0,o.kt)("li",{parentName:"ul"},"Dataset"),(0,o.kt)("li",{parentName:"ul"},"Pipeline"),(0,o.kt)("li",{parentName:"ul"},"Trigger"))),(0,o.kt)("li",{parentName:"ul"},"Data ingest config files (JSON)"),(0,o.kt)("li",{parentName:"ul"},"Azure DevOps CI/CD pipeline (YAML)"),(0,o.kt)("li",{parentName:"ul"},"(optional) Spark job and config file for data quality tests (Python)"),(0,o.kt)("li",{parentName:"ul"},"Template unit tests (Python)"),(0,o.kt)("li",{parentName:"ul"},"Template end-to-end tests (Python, Behave)")),(0,o.kt)("h2",{id:"data-source-pre-requisites"},"Data source pre-requisites"),(0,o.kt)("p",null,"Details required for connecting to the data source will need to be stored securely (i.e. not in the source code) and to be referenced dynamically by the deployment pipeline. This approach also allows for different versions of the data source to be used in different environments (for example non-prod / prod versions). The examples below require the following details to be set for the Azure SQL sample database in each environment:"),(0,o.kt)("h3",{id:"azure-devops-variable"},"Azure DevOps variable"),(0,o.kt)("p",null,"Azure DevOps variables will be accessed dynamically during deployments so is used for details needed to create the linked service in Data Factory."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"sql_connection"),": connection string for the database, for example ",(0,o.kt)("inlineCode",{parentName:"li"},"Data Source=amidostacksdeveuwdesql.database.windows.net;Initial Catalog=exampledb;User ID=user;Integrated Security=False;Encrypt=True;Connection Timeout=30;"))),(0,o.kt)("h3",{id:"key-vault-secret"},"Key Vault secret"),(0,o.kt)("p",null,"The password will need to be accessed dynamically by Data Factory on each connection, therefore should be stored in the Key Vault linked to the factory."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"sql-password"),": password to use with the connection string")),(0,o.kt)("h2",{id:"step-1-create-feature-branch"},"Step 1: Create feature branch"),(0,o.kt)("p",null,"Before creating a new workload using Datastacks, open the project locally and create a new branch for the workload being created, e.g.:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"git checkout -b feat/my-new-ingest-pipeline\n")),(0,o.kt)("h2",{id:"step-2-prepare-the-datastacks-config-file"},"Step 2: Prepare the Datastacks config file"),(0,o.kt)("p",null,"Datastacks requires a YAML config file for generating a new ingest workload - see ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/datastacks#configuration"},"Datastacks configuration")," for further details."),(0,o.kt)("p",null,"Create a new YAML file and populate the values relevant to your new ingest pipeline. The example below will create an ingest workload named ",(0,o.kt)("strong",{parentName:"p"},"Ingest_AzureSql_MyNewExample"),", and connect using the data source connection details as specified in ",(0,o.kt)("a",{parentName:"p",href:"#data-source-pre-requisites"},"Data source pre-requisites")," above."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'#######################\n# Required parameters #\n#######################\n\n# Data pipeline configurations\ndataset_name: AzureSql_MyNewExample\npipeline_description: "Ingest from demo Azure SQL database using ingest config file."\ndata_source_type: azure_sql\ndata_source_password_key_vault_secret_name: sql-password\ndata_source_connection_string_variable_name: sql_connection\n\n# Azure DevOps configurations\nado_variable_groups_nonprod:\n  - amido-stacks-de-pipeline-nonprod\n  - stacks-credentials-nonprod-kv\n\nado_variable_groups_prod:\n  - amido-stacks-de-pipeline-prod\n  - stacks-credentials-prod-kv\n\n#######################\n# Optional parameters #\n#######################\n\n# Workload config\nwindow_start_default: 2010-01-01\nwindow_end_default: 2010-01-31\n\n')),(0,o.kt)("h2",{id:"step-3-generate-project-artifacts-using-datastacks"},"Step 3: Generate project artifacts using Datastacks"),(0,o.kt)("p",null,"Use the ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/datastacks#using-the-datastacks-cli"},"Datastacks CLI")," to generate the artifacts for the new workload, using the prepared config file (replacing ",(0,o.kt)("inlineCode",{parentName:"p"},"path_to_config_file/my_config.yaml")," with the appropriate path).:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'# Activate virtual environment\npoetry shell\n\n# Generate resources for an ingest pipeline (without data quality steps)\ndatastacks generate ingest --config="path_to_config_file/my_config.yaml"\n\n# Generate resources for an ingest pipeline (with added data quality steps)\ndatastacks generate ingest --config="path_to_config_file/my_config.yaml" --data-quality\n')),(0,o.kt)("p",null,"This will add new project artifacts for the workload under ",(0,o.kt)("inlineCode",{parentName:"p"},"de_workloads/ingest/Ingest_AzureSql_MyNewExample"),", based on the ingest workload templates. Review the resources that have been generated."),(0,o.kt)("admonition",{type:"tip"},(0,o.kt)("p",{parentName:"admonition"},"The default ingest workload generated by Datastacks is based upon ingesting from an Azure SQL data source. For the purposes of the ",(0,o.kt)("em",{parentName:"p"},"getting started")," example to you can leave the resources generated as they are. See ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/ingest_data_azure#data-source-types"},"ingest data source types")," for further information on adapting the workload for other data source types.")),(0,o.kt)("admonition",{type:"tip"},(0,o.kt)("p",{parentName:"admonition"},"The default ingest workload contains an example ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/ingest_data_azure#data-factory-triggers"},"tumbling window trigger"),", which defaults to a 'Stopped' state. This is defined in the Terraform resource in ",(0,o.kt)("inlineCode",{parentName:"p"},"data_factory/adf_triggers/tf"),", and can be modified based on your requirements.")),(0,o.kt)("h2",{id:"step-4-update-ingest-configuration"},"Step 4: Update ingest configuration"),(0,o.kt)("p",null,"Configuration of the data that the workload will ingest from the source is specified in the file in the workload's ",(0,o.kt)("inlineCode",{parentName:"p"},"config/ingest_sources/ingest_config.json")," file - see ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/ingest_data_azure#configuration"},"data ingest configuration")," for further details on this file. For the example data source, update the contents of the file with the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "data_source_name": "Ingest_AzureSql_MyNewExample",\n    "data_source_type": "azure_sql",\n    "enabled": true,\n    "ingest_entities": [\n        {\n            "version": 1,\n            "display_name": "movies.movies_metadata",\n            "enabled": true,\n            "schema": "movies",\n            "table": "movies_metadata",\n            "columns": "[adult], [belongs_to_collection], [budget], [genres], [homepage], [id], [imdb_id], [original_language], [original_title], [overview], [popularity], [poster_path], [production_companies], [production_countries], [release_date], [revenue], [runtime], [spoken_languages], [status], [tagline], [title], [video], [vote_average], [vote_count]",\n            "load_type": "full",\n            "delta_date_column": null,\n            "delta_upsert_key": null\n        },\n        {\n            "version": 1,\n            "display_name": "movies.ratings_small",\n            "enabled": true,\n            "schema": "movies",\n            "table": "ratings_small",\n            "columns": "[userId], [movieId], [rating], [timestamp]",\n            "load_type": "delta",\n            "delta_date_column": "DATEADD(SECOND,[timestamp],\'1970-01-01\')",\n            "delta_upsert_key": "[userId], [movieId]"\n        }\n    ]\n}\n')),(0,o.kt)("h2",{id:"step-5-update-end-to-end-tests"},"Step 5: Update end-to-end tests"),(0,o.kt)("p",null,"The ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/testing_data_azure#end-to-end-tests"},"end-to-end tests")," are designed to run the ingest pipeline in a controlled fashion to ensure it functions as expected. Open the test feature file for the workload (",(0,o.kt)("inlineCode",{parentName:"p"},"tests/end_to_end/features/azure_data_ingest.feature"),") and update the parameters to reflect the data entities expected to be ingested. In our example, we will use the entities specified in the config file above, i.e.:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-gherkin"},'|{"window_start" : "2010-01-01", "window_end": "2010-01-31"}|["movies.keywords", "movies.links", "movies.movies_metadata", "movies.ratings_small"]|\n')),(0,o.kt)("h2",{id:"step-6-deploy-new-workload-in-non-production-environment"},"Step 6: Deploy new workload in non-production environment"),(0,o.kt)("p",null,"The generated workload contains a YAML file containing a template Azure DevOps CI/CD pipeline for the workload, named ",(0,o.kt)("inlineCode",{parentName:"p"},"de-ingest-ado-pipeline.yaml"),". This should be added as the definition for a new pipeline in Azure DevOps."),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Sign-in to your Azure DevOps organization and go to your project."),(0,o.kt)("li",{parentName:"ol"},"Go to Pipelines, and then select New pipeline."),(0,o.kt)("li",{parentName:"ol"},"Name the new pipeline to match the name of your new workload, e.g. ",(0,o.kt)("inlineCode",{parentName:"li"},"de-ingest-azuresql-mynewexample"),"."),(0,o.kt)("li",{parentName:"ol"},"For the pipeline definition, specify the YAML file in the project repository feature branch (e.g. ",(0,o.kt)("inlineCode",{parentName:"li"},"de-ingest-ado-pipeline.yaml"),") and save."),(0,o.kt)("li",{parentName:"ol"},"The new pipeline will require access to any Azure DevOps pipeline variable groups specified in the ",(0,o.kt)("a",{parentName:"li",href:"#step-2-prepare-the-datastacks-config-file"},"datastacks config file"),". Under each variable group, go to 'Pipeline permissions' and add the new pipeline."),(0,o.kt)("li",{parentName:"ol"},"Run the new pipeline.")),(0,o.kt)("p",null,"Running this pipeline in Azure DevOps will deploy the artifacts into the non-production (nonprod) environment and run tests. If successful, the generated resources will now be available in the nonprod Ensono Stacks environment."),(0,o.kt)("h2",{id:"step-7-review-deployed-resources"},"Step 7: Review deployed resources"),(0,o.kt)("p",null,"If successful, the new resources will now be deployed into the non-production resource group in Azure - these can be viewed through the ",(0,o.kt)("a",{parentName:"p",href:"https://portal.azure.com/#home"},"Azure Portal")," or CLI."),(0,o.kt)("p",null,"The Azure Data Factory resources can be viewed through the ",(0,o.kt)("a",{parentName:"p",href:"https://adf.azure.com/"},"Data Factory UI"),". You may also wish to run/debug the newly generated pipeline from here (see ",(0,o.kt)("a",{parentName:"p",href:"https://learn.microsoft.com/en-us/azure/data-factory/iterative-development-debugging"},"Microsoft documentation"),")."),(0,o.kt)("admonition",{title:"Updating Data Factory resources",type:"note"},(0,o.kt)("p",{parentName:"admonition"},"The structure of the data platform and Data Factory resources are defined in the project's code repository, and deployed through the Azure DevOps pipelines. Changes to Data Factory resources directly through the UI will lead to them being overwritten when deployment pipelines are next run. See ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/getting_started/dev_quickstart_data_azure#azure-data-factory-development"},"Data Factory development quickstart")," for further information on updating Data Factory resources.")),(0,o.kt)("p",null,"Continue to make any further amendments required to the new workload, re-running the DevOps pipeline as required. If including data quality checks, update the (",(0,o.kt)("inlineCode",{parentName:"p"},"ingest_dq"),") file in the repository with details of checks required on the data (see ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/data_quality_azure#json-configuration-file-for-great-expectations"},"data quality configuration")," for further details)."),(0,o.kt)("h2",{id:"step-8-deploy-new-workload-in-further-environments"},"Step 8: Deploy new workload in further environments"),(0,o.kt)("p",null,"In the example pipeline templates:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Deployment to the non-production (nonprod) environment is triggered on a feature branch when a pull request is open"),(0,o.kt)("li",{parentName:"ul"},"Deployment to the production (prod) environment is triggered on merging to the ",(0,o.kt)("inlineCode",{parentName:"li"},"main")," branch, followed by manual approval of the release step.")),(0,o.kt)("admonition",{type:"tip"},(0,o.kt)("p",{parentName:"admonition"},"It is recommended in any data platform that processes for deploying and releasing across environments should be agreed and documented, ensuring sufficient review and quality assurance of any new workloads. The template CI/CD pipelines provided are based upon two platform environments (nonprod and prod) - but these may be amended depending upon the specific requirements of your project and organisation.")),(0,o.kt)("h2",{id:"next-steps"},"Next steps"),(0,o.kt)("p",null,"Now you have ingested some data into the bronze data lake layer, you can generate a ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/getting_started/processing_pipeline_deployment_azure"},"data processing pipeline")," to transform and model the data."))}g.isMDXComponent=!0}}]);