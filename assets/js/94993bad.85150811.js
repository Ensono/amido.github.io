"use strict";(self.webpackChunkstacks=self.webpackChunkstacks||[]).push([[276],{3905:function(e,t,a){a.d(t,{Zo:function(){return c},kt:function(){return k}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),u=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},c=function(e){var t=u(e.components);return n.createElement(s.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),p=u(a),m=r,k=p["".concat(s,".").concat(m)]||p[m]||d[m]||o;return a?n.createElement(k,i(i({ref:t},c),{},{components:a})):n.createElement(k,i({ref:t},c))}));function k(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:r,i[1]=l;for(var u=2;u<o;u++)i[u]=a[u];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},2034:function(e,t,a){a.r(t),a.d(t,{assets:function(){return c},contentTitle:function(){return s},default:function(){return k},frontMatter:function(){return l},metadata:function(){return u},toc:function(){return p}});var n=a(7462),r=a(3366),o=(a(7294),a(3905)),i=["components"],l={id:"dev_quickstart_data_azure",title:"Local Development Quickstart",sidebar_label:"3. Local Development Quickstart",hide_title:!1,hide_table_of_contents:!1,description:"Quickstart for local development",keywords:["quickstart","development"]},s=void 0,u={unversionedId:"workloads/azure/data/getting_started/dev_quickstart_data_azure",id:"workloads/azure/data/getting_started/dev_quickstart_data_azure",title:"Local Development Quickstart",description:"Quickstart for local development",source:"@site/docs/workloads/azure/data/getting_started/dev_quickstart_data_azure.md",sourceDirName:"workloads/azure/data/getting_started",slug:"/workloads/azure/data/getting_started/dev_quickstart_data_azure",permalink:"/docs/workloads/azure/data/getting_started/dev_quickstart_data_azure",draft:!1,tags:[],version:"current",frontMatter:{id:"dev_quickstart_data_azure",title:"Local Development Quickstart",sidebar_label:"3. Local Development Quickstart",hide_title:!1,hide_table_of_contents:!1,description:"Quickstart for local development",keywords:["quickstart","development"]},sidebar:"docs",previous:{title:"2. Infrastructure Deployment",permalink:"/docs/workloads/azure/data/getting_started/core_data_platform_deployment_azure"},next:{title:"4. Shared Resources Deployment",permalink:"/docs/workloads/azure/data/getting_started/shared_resources_deployment_azure"}},c={},p=[{value:"Environment setup",id:"environment-setup",level:2},{value:"(Optional) Azure connection",id:"optional-azure-connection",level:3},{value:"Running tests",id:"running-tests",level:2},{value:"Unit tests",id:"unit-tests",level:3},{value:"End-to-end tests",id:"end-to-end-tests",level:3},{value:"Code quality checks",id:"code-quality-checks",level:3},{value:"(Optional) PySpark development in Databricks",id:"optional-pyspark-development-in-databricks",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Next steps",id:"next-steps",level:2}],d={toc:p},m="wrapper";function k(e){var t=e.components,a=(0,r.Z)(e,i);return(0,o.kt)(m,(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"This section covers the steps required to start developing a Ensono Stacks Azure Data Platform from your machine:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Make sure you have installed the applications in ",(0,o.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/requirements_data_azure#local-development"},"local development requirements"),"."),(0,o.kt)("li",{parentName:"ol"},"Ensure that Poetry is added to your ",(0,o.kt)("inlineCode",{parentName:"li"},"$PATH"),".")),(0,o.kt)("h2",{id:"environment-setup"},"Environment setup"),(0,o.kt)("p",null,"A Makefile has been created to assist with setting up the development environment. Run:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"make setup_dev_environment\n")),(0,o.kt)("h3",{id:"optional-azure-connection"},"(Optional) Azure connection"),(0,o.kt)("p",null,"In order to interact with Azure resources when developing, including running end-to-end tests, you must:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli"},"Sign in to the Azure CLI")),(0,o.kt)("li",{parentName:"ol"},"Set the following environment variables:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"AZURE_SUBSCRIPTION_ID")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"AZURE_RESOURCE_GROUP_NAME")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"AZURE_DATA_FACTORY_NAME")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"AZURE_REGION_NAME")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"AZURE_STORAGE_ACCOUNT_NAME"))))),(0,o.kt)("h2",{id:"running-tests"},"Running tests"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"make")," commands are provided to assist with running tests while developing locally. See ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/etl_pipelines/testing_data_azure"},"testing")," for further details on these tests."),(0,o.kt)("h3",{id:"unit-tests"},"Unit tests"),(0,o.kt)("p",null,"In order to run unit tests, run the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"make test\n")),(0,o.kt)("h3",{id:"end-to-end-tests"},"End-to-end tests"),(0,o.kt)("p",null,"Running the end-to-end tests will involve executing Data Factory pipelines in Azure. Ensure you have setup the ",(0,o.kt)("a",{parentName:"p",href:"#optional-azure-connection"},"Azure connection")," and run:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"make test_e2e\n")),(0,o.kt)("h3",{id:"code-quality-checks"},"Code quality checks"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://pre-commit.com/"},"Pre-commit")," is used for code quality and linting checks on the project. It can be run using:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"make pre_commit\n")),(0,o.kt)("h2",{id:"optional-pyspark-development-in-databricks"},"(Optional) PySpark development in Databricks"),(0,o.kt)("p",null,"\u2139\ufe0f This sub-section assumes that ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/getting_started/datastacks_deployment_azure"},"Datastacks build & deployment")," has been completed - if you are working through the ",(0,o.kt)("em",{parentName:"p"},"getting started guide")," for the first time, you may skip this section."),(0,o.kt)("p",null,"When developing with PySpark, you may wish to either:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Run scripts locally using a local Spark installation, or"),(0,o.kt)("li",{parentName:"ul"},"Run scripts on a Databricks cluster, through ",(0,o.kt)("a",{parentName:"li",href:"https://learn.microsoft.com/en-us/azure/databricks/repos/"},"Databricks Repos"),".")),(0,o.kt)("p",null,"To run scripts within a Databricks cluster, you will need to:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Install the Datastacks whl file on the cluster, either from:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"The latest deployed version in ",(0,o.kt)("inlineCode",{parentName:"li"},"dbfs:/FileStore/jars/datastacks-latest-py3-none-any.whl"),", or"),(0,o.kt)("li",{parentName:"ul"},"Create a new whl file with the ",(0,o.kt)("inlineCode",{parentName:"li"},"poetry build")," command."))),(0,o.kt)("li",{parentName:"ul"},"Add the additional ",(0,o.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/etl_pipelines/pyspark_utilities#prerequisites"},"environment variables")," required for PySpark development - the values can be set as per the Data Factory linked service (see ",(0,o.kt)("a",{parentName:"li",href:"https://github.com/Ensono/stacks-azure-data/blob/main/de_workloads/shared_resources/data_factory/adf_linked_services.tf"},"adf_linked_services.tf"),")."),(0,o.kt)("li",{parentName:"ul"},"Ensure the user has appropriate permissions for Azure resources required.")),(0,o.kt)("h2",{id:"troubleshooting"},"Troubleshooting"),(0,o.kt)("p",null,"\u2139\ufe0f If you encounter PATH-related issues with Poetry when running the tests, we recommend installing Poetry using\n",(0,o.kt)("a",{parentName:"p",href:"https://python-poetry.org/docs/#installing-with-pipx"},"pipx")," rather than the official installer."),(0,o.kt)("p",null,"\u2139\ufe0f Running end-to-end tests from your local machine may require additional permissions in Azure. If the tests fail whilst clearing up directories, ensure that you have ",(0,o.kt)("a",{parentName:"p",href:"https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#storage-blob-data-contributor"},"Storage Blob Data Contributor")," access applied to your Azure Active Directory subscription. You may also be required to configure the ",(0,o.kt)("a",{parentName:"p",href:"https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security"},"firewall rules")," for the storage account to whitelist your IP address."),(0,o.kt)("h2",{id:"next-steps"},"Next steps"),(0,o.kt)("p",null,"Once you setup your local development environment, you can continue with the Getting Started tutorial by ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/getting_started/shared_resources_deployment_azure"},"deploying the shared resources"),"."))}k.isMDXComponent=!0}}]);