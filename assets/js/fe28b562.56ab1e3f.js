"use strict";(self.webpackChunkstacks=self.webpackChunkstacks||[]).push([[6232],{3905:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return f}});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},g=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=p(n),g=i,f=u["".concat(l,".").concat(g)]||u[g]||c[g]||r;return n?a.createElement(f,o(o({ref:t},d),{},{components:n})):a.createElement(f,o({ref:t},d))}));function f(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=g;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:i,o[1]=s;for(var p=2;p<r;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}g.displayName="MDXCreateElement"},6869:function(e,t,n){n.r(t),n.d(t,{contentTitle:function(){return l},default:function(){return g},frontMatter:function(){return s},metadata:function(){return p},toc:function(){return d}});var a=n(7462),i=n(3366),r=(n(7294),n(3905)),o=["components"],s={id:"ingest_data_azure",title:"Data Ingestion",sidebar_label:"Data Ingestion",hide_title:!1,hide_table_of_contents:!1,description:"Data ingestion pipeline",keywords:["ingest","adf","etl"]},l=void 0,p={unversionedId:"workloads/azure/data/etl_pipelines/ingest_data_azure",id:"workloads/azure/data/etl_pipelines/ingest_data_azure",isDocsHomePage:!1,title:"Data Ingestion",description:"Data ingestion pipeline",source:"@site/docs/workloads/azure/data/etl_pipelines/ingest_data_azure.md",sourceDirName:"workloads/azure/data/etl_pipelines",slug:"/workloads/azure/data/etl_pipelines/ingest_data_azure",permalink:"/docs/workloads/azure/data/etl_pipelines/ingest_data_azure",tags:[],version:"current",frontMatter:{id:"ingest_data_azure",title:"Data Ingestion",sidebar_label:"Data Ingestion",hide_title:!1,hide_table_of_contents:!1,description:"Data ingestion pipeline",keywords:["ingest","adf","etl"]},sidebar:"docs",previous:{title:"ETL Overview",permalink:"/docs/workloads/azure/data/etl_pipelines/etl_intro_data_azure"},next:{title:"Silver Pipeline",permalink:"/docs/workloads/azure/data/etl_pipelines/silver_data_azure"}},d=[{value:"Pipeline overview",id:"pipeline-overview",children:[],level:2},{value:"Configuration",id:"configuration",children:[{value:"Data Factory pipeline design",id:"data-factory-pipeline-design",children:[],level:3}],level:2}],u={toc:d},c="wrapper";function g(e){var t=e.components,s=(0,i.Z)(e,o);return(0,r.kt)(c,(0,a.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"The solution contains a sample Azure Data Factory pipeline for ingesting data from a sample data\nsource (Azure SQL) and loading data into the data lake 'landing' zone."),(0,r.kt)("p",null,"Link to the pipeline: ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ensono/stacks-azure-data/tree/main/de_workloads/ingest/Ingest_AzureSql_Example"},"stacks-azure-data/de_workloads/ingest"),"."),(0,r.kt)("h2",{id:"pipeline-overview"},"Pipeline overview"),(0,r.kt)("p",null,"The diagram below gives an overview of the ingestion pipeline design."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"ADF_IngestPipelineDesign.png",src:n(6050).Z})),(0,r.kt)("h2",{id:"configuration"},"Configuration"),(0,r.kt)("p",null,"The ingest process is designed around reusable, metadata-driven pipelines. This means once\nan initial data pipeline is created for a given data source, additional entities from the same data\nsource can be added or modified just by updating a configuration file. These configuration files are\nstored in the pipeline's ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ensono/stacks-azure-data/tree/main/de_workloads/ingest/Ingest_AzureSql_Example/config"},"config")," directory."),(0,r.kt)("p",null,"JSON format is used for the configuration files. Our blueprint includes a sample configuration definition for the data ingestion sources\n(",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ensono/stacks-azure-data/blob/main/de_workloads/ingest/Ingest_AzureSql_Example/config/ingest_sources/Ingest_AzureSql_Example.json"},"Ingest_AzureSql_Example.json"),")\nand its schema (",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ensono/stacks-azure-data/blob/main/de_workloads/ingest/Ingest_AzureSql_Example/config/schema/ingest_config_schema.json"},"ingest_config_schema.json"),")."),(0,r.kt)("p",null,"The sample ingest pipeline is based around an Azure SQL data source, however the approach used should be adaptable for most other data source types with minimal modifications. Different data data source types would be expected to have the same JSON keys, except for under ",(0,r.kt)("inlineCode",{parentName:"p"},"ingest_entities"),",\nwhere different keys will be required dependent on the data source type."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/ensono/stacks-azure-data/tree/main/de_workloads/ingest/Ingest_AzureSql_Example/tests/unit"},"Unit tests"),"\nare provided to ensure the config files remain valid against the schema. See the descriptions of the\nexample JSON config file below:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'{\n    "data_source_name": "example_azuresql_1",  # Identifier of the data source - must be unique\n    "data_source_type": "azure_sql",           # Data source type\n    "enabled": true,                           # Boolean flag to enable / disable the data source from being ingested\n    "ingest_entities": [                       # Array of entities to be ingested from the source\n        {\n            "version": 1,                      # Version number - increment this if the entity\'s schema changes\n            "display_name": "SalesLT.Product", # Name to identify the entity - must be unique per data source\n            "enabled": true,                   # Boolean flag to enable / disable the entity from being ingested\n            "schema": "SalesLT",               # (SQL sources only) Database schema\n            "table": "Product",                # (SQL sources only) Database table\n            "columns": "*",                    # (SQL sources only) Columns to select\n            "load_type": "delta",              # (SQL sources only) Full or delta load. If delta load selected, then also include the following keys\n            "delta_date_column": "ModifiedDate",  # (SQL sources only, delta load) Date column to use for filtering the date range\n            "delta_upsert_key": "SalesOrderID"    # (SQL sources only, delta load) Primary key for determining updated columns in a delta load\n        }\n    ]\n}\n')),(0,r.kt)("p",null,"These configuration files will be referenced each time an ingestion pipeline\nis triggered in Data Factory, and all entities will be ingested. To disable a particular ingest\nsource or entity without removing it, you can set ",(0,r.kt)("inlineCode",{parentName:"p"},'"enabled": false')," \u2013 these will be ignored by\nthe Data Factory pipeline."),(0,r.kt)("h3",{id:"data-factory-pipeline-design"},"Data Factory pipeline design"),(0,r.kt)("p",null,"The provided sample pipelines give an example of a data ingest process from source to the data lake.\nThe pipelines folder is structured as follows:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"ADF_IngestPipelinesList.png",src:n(2359).Z})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"Ingest")," contains ingest pipelines specific to the given data source. The naming convention for\nthese pipelines is ",(0,r.kt)("inlineCode",{parentName:"li"},"Ingest_{SourceType}_{SourceName}"),". These are the parent pipelines that would be\ntriggered on a recurring basis to ingest from a data source. All pipelines have their equivalents\nthat include Data Quality validations. Depending on your particular needs, you can deploy each of\nthe pipelines with or without this additional Data Quality step. ",(0,r.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/etl_pipelines/data_quality_azure"},"Further information on Data Quality"),"."),(0,r.kt)("li",{parentName:"ul"},"The pipelines within ",(0,r.kt)("inlineCode",{parentName:"li"},"Utilities")," are reusable and referenced by other pipelines. They are not\nmeant to be triggered independently. These are defined within the ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/ensono/stacks-azure-data/tree/main/de_workloads/shared_resources"},"shared_resources")," for the project.")),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"Ingest_AzureSql_Example")," pipeline consists of the following steps:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"ADF_Ingest_AzureSql_Example.png",src:n(9392).Z})),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"Get_Ingest_Config"),": Calls the utility pipeline, passing the data source name as a parameter.\nThis will return the configuration required for the given data source."),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"For_Each_Ingest_Entity"),": Loop through each ingest entity performing the following steps:",(0,r.kt)("ol",{parentName:"li"},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"Generate_Ingest_Query"),": Generates a SQL query to extract the data from a required time range,\naccording to the provided configuration. Depending on the load type, one of the two scenarios\nbelow will be applied:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Full extraction loads all available data for a given set of columns,"),(0,r.kt)("li",{parentName:"ul"},"Delta queries contain a ",(0,r.kt)("inlineCode",{parentName:"li"},"WHERE")," clause to restrict the date range loaded."))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"SQL_to_ADLS"),": Execute the SQL query against the data source, and copy the results to the\nAzure Data Lake storage landing container under the appropriate path (data is validated using\nADF's built-in data validation capability).")))),(0,r.kt)("p",null,"The following picture shows the two possibilities of full vs delta extraction in ",(0,r.kt)("inlineCode",{parentName:"p"},"Generate_Ingest_Query"),":"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"ADF_IngestGenerateIngestQuery.png",src:n(5047).Z})))}g.isMDXComponent=!0},5047:function(e,t,n){t.Z=n.p+"assets/images/ADF_IngestGenerateIngestQuery-875a13c99e0cd363046554d3c37182d0.png"},6050:function(e,t,n){t.Z=n.p+"assets/images/ADF_IngestPipelineDesign-1c61616f6fb3baa0101682c3e522412d.png"},2359:function(e,t,n){t.Z=n.p+"assets/images/ADF_IngestPipelinesList-e62d3f121f131951eb307ac286be4129.png"},9392:function(e,t,n){t.Z=n.p+"assets/images/ADF_Ingest_AzureSql_Example-66feb8518733339a00c4fa1aa4e0784b.png"}}]);