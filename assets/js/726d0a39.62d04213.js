"use strict";(self.webpackChunkstacks=self.webpackChunkstacks||[]).push([[508],{3905:function(e,t,r){r.d(t,{Zo:function(){return c},kt:function(){return y}});var n=r(7294);function o(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function a(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?a(Object(r),!0).forEach((function(t){o(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,n,o=function(e,t){if(null==e)return{};var r,n,o={},a=Object.keys(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||(o[r]=e[r]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(o[r]=e[r])}return o}var u=n.createContext({}),l=function(e){var t=n.useContext(u),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},c=function(e){var t=l(e.components);return n.createElement(u.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},f=n.forwardRef((function(e,t){var r=e.components,o=e.mdxType,a=e.originalType,u=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=l(r),f=o,y=p["".concat(u,".").concat(f)]||p[f]||d[f]||a;return r?n.createElement(y,i(i({ref:t},c),{},{components:r})):n.createElement(y,i({ref:t},c))}));function y(e,t){var r=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=r.length,i=new Array(a);i[0]=f;var s={};for(var u in t)hasOwnProperty.call(t,u)&&(s[u]=t[u]);s.originalType=e,s[p]="string"==typeof e?e:o,i[1]=s;for(var l=2;l<a;l++)i[l]=r[l];return n.createElement.apply(null,i)}return n.createElement.apply(null,r)}f.displayName="MDXCreateElement"},2465:function(e,t,r){r.r(t),r.d(t,{contentTitle:function(){return u},default:function(){return f},frontMatter:function(){return s},metadata:function(){return l},toc:function(){return c}});var n=r(7462),o=r(3366),a=(r(7294),r(3905)),i=["components"],s={id:"repository_data_azure",title:"Repository structure",sidebar_label:"Repository",hide_title:!1,hide_table_of_contents:!1,description:"Repository structure explained",keywords:["data","python","repository"]},u=void 0,l={unversionedId:"workloads/azure/data/repository_data_azure",id:"workloads/azure/data/repository_data_azure",isDocsHomePage:!1,title:"Repository structure",description:"Repository structure explained",source:"@site/docs/workloads/azure/data/repository_data_azure.md",sourceDirName:"workloads/azure/data",slug:"/workloads/azure/data/repository_data_azure",permalink:"/docs/workloads/azure/data/repository_data_azure",tags:[],version:"current",frontMatter:{id:"repository_data_azure",title:"Repository structure",sidebar_label:"Repository",hide_title:!1,hide_table_of_contents:!1,description:"Repository structure explained",keywords:["data","python","repository"]},sidebar:"docs",previous:{title:"Architecture Overview",permalink:"/docs/workloads/azure/data/intro_data_azure"},next:{title:"Infrastructure",permalink:"/docs/workloads/azure/data/components/infrastructure/core_data_platform_azure"}},c=[],p={toc:c},d="wrapper";function f(e){var t=e.components,r=(0,o.Z)(e,i);return(0,a.kt)(d,(0,n.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"Structure of ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/amido/stacks-azure-data"},"stacks-azure-data")," repository:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-md"},"stacks-azure-data\n\u251c\u2500\u2500 build           # Azure DevOps pipelines configuration for building and deploying the core infrastructure\n\u251c\u2500\u2500 data_processing # Azure Data Factory ETL pipelines, leveraging Databricks for data transformations\n\u2502   \u251c\u2500\u2500 config      # Configuration files (uploaded to blob storage)\n\u2502   \u251c\u2500\u2500 jobs        # Data processing pipelines with optional Data Quality checks\n\u2502   \u2502   \u251c\u2500\u2500 gold    # Bronze to Silver layer transformations\n\u2502   \u2502   \u251c\u2500\u2500 silver  # Silver to Gold layer transformations\n\u251c\u2500\u2500 de_build        # Azure DevOps pipelines configuration for building and deploying ADF pipelines\n\u251c\u2500\u2500 deploy          # TF modules to deploy core Azure resources (used by `build` directory)\n\u251c\u2500\u2500 docs            # Documentation\n\u251c\u2500\u2500 ingest          # Pipeline utilizing ADF for data ingestion, with optional Data Quality checks\n\u2502   \u251c\u2500\u2500 config      # Configuration files used by ETL and DQ processes (uploaded to blob storage)\n\u2502   \u251c\u2500\u2500 jobs\n\u2502   \u2502   \u251c\u2500\u2500 Generate_Ingest_Query   # Helper utility used in the ingestion pipeline\n\u2502   \u2502   \u251c\u2500\u2500 Get_Ingest_Config       # Helper utility used in the ingestion pipeline\n\u2502   \u2502   \u251c\u2500\u2500 Ingest_AzureSql_Example # Data ingestion pipeline with optional Data Quality checks\n\u251c\u2500\u2500 pysparkle       # Python library built to streamline data processing; packaged and uploaded to DBFS\n\u251c\u2500\u2500 utils           # Python utilities package used across solution for local testing\n\u251c\u2500\u2500 .flake8                 # Configuration for Flake8 linting\n\u251c\u2500\u2500 .pre-commit-config.yaml # Configuration for pre-commit hooks\n\u251c\u2500\u2500 Makefile                # Includes commands for environment setup\n\u251c\u2500\u2500 pyproject.toml          # Project dependencies\n\u2514\u2500\u2500 README.md               # This file.\n")))}f.isMDXComponent=!0}}]);