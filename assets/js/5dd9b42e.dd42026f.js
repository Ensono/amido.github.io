"use strict";(self.webpackChunkstacks=self.webpackChunkstacks||[]).push([[9941],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return k}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=p(a),m=r,k=u["".concat(l,".").concat(m)]||u[m]||c[m]||o;return a?n.createElement(k,i(i({ref:t},d),{},{components:a})):n.createElement(k,i({ref:t},d))}));function k(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},3173:function(e,t,a){a.r(t),a.d(t,{assets:function(){return d},contentTitle:function(){return l},default:function(){return k},frontMatter:function(){return s},metadata:function(){return p},toc:function(){return u}});var n=a(7462),r=a(3366),o=(a(7294),a(3905)),i=["components"],s={id:"datastacks_deployment_azure",title:"Datastacks Build & Deployment",sidebar_label:"5. Datastacks Build & Deployment",hide_title:!1,hide_table_of_contents:!1,description:"Build and deployment of Datastacks utilities library",keywords:["data","infrastructure","databricks","python","cicd"]},l=void 0,p={unversionedId:"workloads/azure/data/getting_started/datastacks_deployment_azure",id:"workloads/azure/data/getting_started/datastacks_deployment_azure",title:"Datastacks Build & Deployment",description:"Build and deployment of Datastacks utilities library",source:"@site/docs/workloads/azure/data/getting_started/datastacks_deployment_azure.md",sourceDirName:"workloads/azure/data/getting_started",slug:"/workloads/azure/data/getting_started/datastacks_deployment_azure",permalink:"/docs/workloads/azure/data/getting_started/datastacks_deployment_azure",draft:!1,tags:[],version:"current",frontMatter:{id:"datastacks_deployment_azure",title:"Datastacks Build & Deployment",sidebar_label:"5. Datastacks Build & Deployment",hide_title:!1,hide_table_of_contents:!1,description:"Build and deployment of Datastacks utilities library",keywords:["data","infrastructure","databricks","python","cicd"]},sidebar:"docs",previous:{title:"4. Shared Resources Deployment",permalink:"/docs/workloads/azure/data/getting_started/shared_resources_deployment_azure"},next:{title:"6. Example Data Source",permalink:"/docs/workloads/azure/data/getting_started/example_data_source"}},d={},u=[{value:"Step 1: Create feature branch",id:"step-1-create-feature-branch",level:2},{value:"Step 2: Add a Datastacks pipeline in Azure DevOps",id:"step-2-add-a-datastacks-pipeline-in-azure-devops",level:2},{value:"Step 3: Deploy Datastacks in non-production environment",id:"step-3-deploy-datastacks-in-non-production-environment",level:2},{value:"Step 4: Deploy shared resources in further environments",id:"step-4-deploy-shared-resources-in-further-environments",level:2},{value:"Next steps",id:"next-steps",level:2}],c={toc:u},m="wrapper";function k(e){var t=e.components,a=(0,r.Z)(e,i);return(0,o.kt)(m,(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"This section provides an overview of the build and deployment of the ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/etl_pipelines/pyspark_utilities"},"Datastacks Python library"),"."),(0,o.kt)("p",null,"Datastacks provides a range of utilities designed to support the development of data engineering workloads. The steps in this section are to setup a deployment pipeline, which:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Build and package Datastacks as a Python wheel."),(0,o.kt)("li",{parentName:"ul"},"Load the wheel into Databricks' DBFS, so it is accessible to jobs running in Databricks.")),(0,o.kt)("p",null,"This guide assumes the following are in place:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A ",(0,o.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/core_data_platform_deployment_azure"},"deployed Ensono Stacks data platform")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/dev_quickstart_data_azure"},"Development environment set up")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/shared_resources_deployment_azure"},"Deployed shared resources"))),(0,o.kt)("h2",{id:"step-1-create-feature-branch"},"Step 1: Create feature branch"),(0,o.kt)("p",null,"Open the project locally and create a new feature branch, e.g.:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"git checkout -b feat/datastacks-deployment\n")),(0,o.kt)("h2",{id:"step-2-add-a-datastacks-pipeline-in-azure-devops"},"Step 2: Add a Datastacks pipeline in Azure DevOps"),(0,o.kt)("p",null,"The Datastacks utility resources are located within the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/ensono/stacks-azure-data/tree/main/datastacks"},"datastacks")," directory. This directory contains a YAML file ",(0,o.kt)("inlineCode",{parentName:"p"},"datastacks-pipeline.yml")," containing a template Azure DevOps CI/CD pipeline for building and deploying the Datastacks Python library.\nThis YAML file should be added as the definition for a new pipeline in Azure DevOps."),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Sign-in to your Azure DevOps organization and go to your project."),(0,o.kt)("li",{parentName:"ol"},"Go to Pipelines, and then select ",(0,o.kt)("strong",{parentName:"li"},"New pipeline"),"."),(0,o.kt)("li",{parentName:"ol"},"Name the new pipeline, e.g. ",(0,o.kt)("inlineCode",{parentName:"li"},"datastacks-build-deploy"),"."),(0,o.kt)("li",{parentName:"ol"},"For the pipeline definition, specify the YAML file in the project repository feature branch (",(0,o.kt)("inlineCode",{parentName:"li"},"datastacks-pipeline.yml"),") and save."),(0,o.kt)("li",{parentName:"ol"},"The new pipeline will require access to any Azure DevOps pipeline variable groups specified in the pipeline YAML. Under each variable group, go to 'Pipeline permissions' and add the pipeline.")),(0,o.kt)("h2",{id:"step-3-deploy-datastacks-in-non-production-environment"},"Step 3: Deploy Datastacks in non-production environment"),(0,o.kt)("p",null,"Run the pipeline configured in Step 2 to commence the build and deployment process."),(0,o.kt)("p",null,"Running this pipeline in Azure DevOps will initiate the build and deployment of the Datastacks library into the non-production (nonprod) environment. It's important to monitor the progress of this deployment to ensure its success. You can track the progress and status of the deployment within the Pipelines section of Azure DevOps."),(0,o.kt)("p",null,"\u2139\ufe0f Once deployed, Datastacks Python utilities will be accessible to scripts running in Databricks. The workloads created in the following examples will utilise this, or refer to ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/getting_started/dev_quickstart_data_azure#optional-pyspark-development-in-databricks"},"PySpark development in Databricks"),"."),(0,o.kt)("h2",{id:"step-4-deploy-shared-resources-in-further-environments"},"Step 4: Deploy shared resources in further environments"),(0,o.kt)("p",null,"By default Ensono Stacks provides a framework for managing the platform across two environments - nonprod and prod.\nThe template CI/CD pipelines provided are based upon these two platform environments, but these may be amended depending upon the specific requirements of your project and organisation."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Deployment to the non-production (nonprod) environment is triggered on a feature branch when a pull request is open"),(0,o.kt)("li",{parentName:"ul"},"Deployment to the production (prod) environment is triggered on merging to the ",(0,o.kt)("inlineCode",{parentName:"li"},"main")," branch, followed by manual approval of the release step.")),(0,o.kt)("h2",{id:"next-steps"},"Next steps"),(0,o.kt)("p",null,"Now Datastacks Python utilities have been deployed you may now ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/getting_started/ingest_pipeline_deployment_azure"},"generate a new data ingest pipeline")," (optionally implementing the ",(0,o.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/getting_started/example_data_source"},"example data source")," beforehand)."))}k.isMDXComponent=!0}}]);