"use strict";(self.webpackChunkstacks=self.webpackChunkstacks||[]).push([[6225],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return f}});var r=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var l=r.createContext({}),p=function(e){var t=r.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=p(e.components);return r.createElement(l.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},k=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),c=p(a),k=n,f=c["".concat(l,".").concat(k)]||c[k]||u[k]||i;return a?r.createElement(f,o(o({ref:t},d),{},{components:a})):r.createElement(f,o({ref:t},d))}));function f(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,o=new Array(i);o[0]=k;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[c]="string"==typeof e?e:n,o[1]=s;for(var p=2;p<i;p++)o[p]=a[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}k.displayName="MDXCreateElement"},8260:function(e,t,a){a.r(t),a.d(t,{assets:function(){return d},contentTitle:function(){return l},default:function(){return f},frontMatter:function(){return s},metadata:function(){return p},toc:function(){return c}});var r=a(7462),n=a(3366),i=(a(7294),a(3905)),o=["components"],s={id:"data_processing",title:"Data Processing Workloads",sidebar_label:"Data Processing Workloads",hide_title:!1,hide_table_of_contents:!1,description:"Data Processing Workloads",keywords:["silver","adf","etl","databricks"]},l=void 0,p={unversionedId:"workloads/azure/data/etl_pipelines/data_processing",id:"workloads/azure/data/etl_pipelines/data_processing",title:"Data Processing Workloads",description:"Data Processing Workloads",source:"@site/docs/workloads/azure/data/etl_pipelines/data_processing.md",sourceDirName:"workloads/azure/data/etl_pipelines",slug:"/workloads/azure/data/etl_pipelines/data_processing",permalink:"/docs/workloads/azure/data/etl_pipelines/data_processing",draft:!1,tags:[],version:"current",frontMatter:{id:"data_processing",title:"Data Processing Workloads",sidebar_label:"Data Processing Workloads",hide_title:!1,hide_table_of_contents:!1,description:"Data Processing Workloads",keywords:["silver","adf","etl","databricks"]},sidebar:"docs",previous:{title:"Data Ingest Workloads",permalink:"/docs/workloads/azure/data/etl_pipelines/ingest_data_azure"},next:{title:"Data Quality",permalink:"/docs/workloads/azure/data/etl_pipelines/data_quality_azure"}},d={},c=[{value:"Data processing pipeline overview",id:"data-processing-pipeline-overview",level:2},{value:"Data Factory pipeline design",id:"data-factory-pipeline-design",level:3}],u={toc:c},k="wrapper";function f(e){var t=e.components,s=(0,n.Z)(e,o);return(0,i.kt)(k,(0,r.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Data processing workloads in Ensono Stacks are jobs which:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Take data in the data lake as input (this can be various formats e.g. CSV, Parquet, JSON, Delta)."),(0,i.kt)("li",{parentName:"ol"},"Perform some form of data transformation / cleansing / modelling / aggregation over the data."),(0,i.kt)("li",{parentName:"ol"},"Output the into the data lake into a structured ",(0,i.kt)("a",{parentName:"li",href:"https://delta.io/"},"Delta Lake")," format.")),(0,i.kt)("p",null,"Data processing workloads are based upon running Apache Spark / Python jobs on Databricks. These workloads may be used for various levels of data transformation and preparation within the data lake. Within the ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/etl_pipelines/etl_intro_data_azure#medallion-architecture"},"medallion architecture")," these will include:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Bronze to Silver"),(0,i.kt)("li",{parentName:"ul"},"Silver to Gold")),(0,i.kt)("p",null,"Processing workloads will utilise ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/etl_pipelines/pyspark_utilities"},"Datastacks' PySpark utilities"),", to support and standardise common tasks. Similar to ingest workloads, data processing workloads can also optionally include a ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/etl_pipelines/data_quality_azure"},"Data Quality validations")," step."),(0,i.kt)("p",null,"The following example data processing workloads are included for reference:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/Ensono/stacks-azure-data/tree/main/de_workloads/data_processing/silver_movies_example"},"silver_movies_example"),": Performs Bronze to Silver layer data processing over the ",(0,i.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/example_data_source"},"example dataset"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/Ensono/stacks-azure-data/tree/main/de_workloads/data_processing/gold_movies_example"},"gold_movies_example"),": Aggregates and joins data from the Silver layer example above to create a Gold layer dataset.")),(0,i.kt)("h2",{id:"data-processing-pipeline-overview"},"Data processing pipeline overview"),(0,i.kt)("p",null,"Within Stacks, processing activities are performed using Python PySpark jobs. These jobs are orchestrated by pipelines in Data Factory, and executed in Databricks. Using PySpark jobs - as opposed to notebooks - gives full control over the processing activities (for example ensuring thorough ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/etl_pipelines/testing_data_azure"},"test coverage")," and quality control)."),(0,i.kt)("p",null,"The diagram below gives an example of a data processing data pipeline in Data Factory."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"ADF_SilverGoldPipelineDesign.png",src:a(6787).Z,width:"2775",height:"1151"})),(0,i.kt)("p",null,"The Python PySpark script executed as part of a data workload is kept inside the ",(0,i.kt)("inlineCode",{parentName:"p"},"spark_jobs")," directory for the workload. This job will utilise the ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/etl_pipelines/datastacks"},"Datastacks library"),", which provides a wealth of reusable utilities to assist with data transformations and loading data from/into to the data lake."),(0,i.kt)("h3",{id:"data-factory-pipeline-design"},"Data Factory pipeline design"),(0,i.kt)("p",null,"Within Data Factory, the processing pipelines are kept within the ",(0,i.kt)("inlineCode",{parentName:"p"},"Process")," folder:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"ADF_SilverPipelinesList.png",src:a(5854).Z,width:"283",height:"243"})),(0,i.kt)("p",null,"In Data Factory a data processing pipeline can be as simple as this example:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"ADF_Silver.png",src:a(3075).Z,width:"789",height:"567"})),(0,i.kt)("p",null,"It contains just one step - Python Databricks, configured to run a ",(0,i.kt)("inlineCode",{parentName:"p"},"silver.py")," script, which gets deployed to DBFS\n(",(0,i.kt)("inlineCode",{parentName:"p"},"dbfs:/FileStore/scripts/silver/silver.py"),"). The Datastacks package and library is deployed to DBFS, and made available to the cluster."))}f.isMDXComponent=!0},6787:function(e,t,a){t.Z=a.p+"assets/images/ADF_SilverGoldPipelineDesign-45f67d23a527feb7cca909593202ce35.png"},5854:function(e,t,a){t.Z=a.p+"assets/images/ADF_SilverPipelinesList-ad113e315740f36f95fedaf0df540032.png"},3075:function(e,t,a){t.Z=a.p+"assets/images/ADF_silver-ed448a6a5a99d4709d2c4c6f2bb1394d.png"}}]);