"use strict";(self.webpackChunkstacks=self.webpackChunkstacks||[]).push([[9609],{3905:function(e,a,t){t.d(a,{Zo:function(){return p},kt:function(){return k}});var r=t(7294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,r,n=function(e,a){if(null==e)return{};var t,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var l=r.createContext({}),d=function(e){var a=r.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},p=function(e){var a=d(e.components);return r.createElement(l.Provider,{value:a},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},g=r.forwardRef((function(e,a){var t=e.components,n=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),c=d(t),g=n,k=c["".concat(l,".").concat(g)]||c[g]||u[g]||i;return t?r.createElement(k,o(o({ref:a},p),{},{components:t})):r.createElement(k,o({ref:a},p))}));function k(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var i=t.length,o=new Array(i);o[0]=g;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s[c]="string"==typeof e?e:n,o[1]=s;for(var d=2;d<i;d++)o[d]=t[d];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}g.displayName="MDXCreateElement"},1242:function(e,a,t){t.r(a),t.d(a,{assets:function(){return p},contentTitle:function(){return l},default:function(){return k},frontMatter:function(){return s},metadata:function(){return d},toc:function(){return c}});var r=t(7462),n=t(3366),i=(t(7294),t(3905)),o=["components"],s={id:"data_processing",title:"Data Processing Workloads",sidebar_label:"Data Processing Workloads",hide_title:!1,hide_table_of_contents:!1,description:"Data Processing Workloads",keywords:["silver","adf","etl","databricks"]},l=void 0,d={unversionedId:"workloads/azure/data/data_engineering/data_processing",id:"workloads/azure/data/data_engineering/data_processing",title:"Data Processing Workloads",description:"Data Processing Workloads",source:"@site/docs/workloads/azure/data/data_engineering/data_processing.md",sourceDirName:"workloads/azure/data/data_engineering",slug:"/workloads/azure/data/data_engineering/data_processing",permalink:"/docs/workloads/azure/data/data_engineering/data_processing",draft:!1,tags:[],version:"current",frontMatter:{id:"data_processing",title:"Data Processing Workloads",sidebar_label:"Data Processing Workloads",hide_title:!1,hide_table_of_contents:!1,description:"Data Processing Workloads",keywords:["silver","adf","etl","databricks"]},sidebar:"docs",previous:{title:"Data Ingest Workloads",permalink:"/docs/workloads/azure/data/data_engineering/ingest_data_azure"},next:{title:"Data Quality",permalink:"/docs/workloads/azure/data/data_engineering/data_quality_azure"}},p={},c=[{value:"Data processing pipeline overview",id:"data-processing-pipeline-overview",level:2},{value:"Data Factory pipeline design",id:"data-factory-pipeline-design",level:3}],u={toc:c},g="wrapper";function k(e){var a=e.components,s=(0,n.Z)(e,o);return(0,i.kt)(g,(0,r.Z)({},u,s,{components:a,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Data processing workloads in Ensono Stacks are jobs which:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Take data in the data lake as input (this can be various formats e.g. CSV, Parquet, JSON, Delta)."),(0,i.kt)("li",{parentName:"ol"},"Perform some form of data transformation / cleansing / modelling / aggregation over the data."),(0,i.kt)("li",{parentName:"ol"},"Output the into the data lake into a structured ",(0,i.kt)("a",{parentName:"li",href:"https://delta.io/"},"Delta Lake")," format.")),(0,i.kt)("p",null,"Data processing workloads are based upon running Apache Spark / Python jobs on Databricks. These workloads may be used for various levels of data transformation and preparation within the data lake. Within the ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/data_engineering_intro_azure#medallion-architecture"},"medallion architecture")," these will include:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Bronze to Silver"),(0,i.kt)("li",{parentName:"ul"},"Silver to Gold")),(0,i.kt)("p",null,"Processing workloads will utilise ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/pyspark_utilities"},"Datastacks' PySpark utilities"),", to support and standardise common tasks. Similar to ingest workloads, data processing workloads can also optionally include a ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/data_quality_azure"},"Data Quality validations")," step."),(0,i.kt)("p",null,"The following example data processing workloads are included for reference:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/Ensono/stacks-azure-data/tree/main/de_workloads/data_processing/silver_movies_example"},"silver_movies_example"),": Performs Bronze to Silver layer data processing over the ",(0,i.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/example_data_source"},"example dataset"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/Ensono/stacks-azure-data/tree/main/de_workloads/data_processing/gold_movies_example"},"gold_movies_example"),": Aggregates and joins data from the Silver layer example above to create a Gold layer dataset.")),(0,i.kt)("h2",{id:"data-processing-pipeline-overview"},"Data processing pipeline overview"),(0,i.kt)("p",null,"Within Stacks, processing activities are performed using Python PySpark jobs. These jobs are orchestrated by pipelines in Data Factory, and executed in Databricks. Using PySpark jobs - as opposed to notebooks - gives full control over the processing activities (for example ensuring thorough ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/testing_data_azure"},"test coverage")," and quality control)."),(0,i.kt)("p",null,"The diagram below gives an example of a data processing data pipeline in Data Factory."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"ADF_SilverGoldPipelineDesign.png",src:t(6787).Z,width:"2775",height:"1151"})),(0,i.kt)("p",null,"The Python PySpark script executed as part of a data workload is kept inside the ",(0,i.kt)("inlineCode",{parentName:"p"},"spark_jobs")," directory for the workload. This job will utilise the ",(0,i.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/data_engineering/datastacks"},"Datastacks library"),", which provides a wealth of reusable utilities to assist with data transformations and loading data from/into to the data lake."),(0,i.kt)("h3",{id:"data-factory-pipeline-design"},"Data Factory pipeline design"),(0,i.kt)("p",null,"Within Data Factory, the processing pipelines are kept within the ",(0,i.kt)("inlineCode",{parentName:"p"},"Process")," folder:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"ADF_SilverPipelinesList.png",src:t(5854).Z,width:"283",height:"243"})),(0,i.kt)("p",null,"In Data Factory a data processing pipeline can be as simple as this example:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"ADF_Silver.png",src:t(3075).Z,width:"789",height:"567"})),(0,i.kt)("p",null,"It contains just one step - Python Databricks, configured to run a ",(0,i.kt)("inlineCode",{parentName:"p"},"silver.py")," script, which gets deployed to DBFS\n(",(0,i.kt)("inlineCode",{parentName:"p"},"dbfs:/FileStore/scripts/silver/silver.py"),"). The Datastacks package and library is deployed to DBFS, and made available to the cluster."))}k.isMDXComponent=!0},6787:function(e,a,t){a.Z=t.p+"assets/images/ADF_SilverGoldPipelineDesign-45f67d23a527feb7cca909593202ce35.png"},5854:function(e,a,t){a.Z=t.p+"assets/images/ADF_SilverPipelinesList-ad113e315740f36f95fedaf0df540032.png"},3075:function(e,a,t){a.Z=t.p+"assets/images/ADF_silver-ed448a6a5a99d4709d2c4c6f2bb1394d.png"}}]);