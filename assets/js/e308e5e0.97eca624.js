"use strict";(self.webpackChunkstacks=self.webpackChunkstacks||[]).push([[910],{3905:function(t,e,a){a.d(e,{Zo:function(){return p},kt:function(){return g}});var n=a(7294);function r(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function l(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,n)}return a}function i(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?l(Object(a),!0).forEach((function(e){r(t,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(a,e))}))}return t}function o(t,e){if(null==t)return{};var a,n,r=function(t,e){if(null==t)return{};var a,n,r={},l=Object.keys(t);for(n=0;n<l.length;n++)a=l[n],e.indexOf(a)>=0||(r[a]=t[a]);return r}(t,e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(t);for(n=0;n<l.length;n++)a=l[n],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(r[a]=t[a])}return r}var d=n.createContext({}),s=function(t){var e=n.useContext(d),a=e;return t&&(a="function"==typeof t?t(e):i(i({},e),t)),a},p=function(t){var e=s(t.components);return n.createElement(d.Provider,{value:e},t.children)},u="mdxType",k={inlineCode:"code",wrapper:function(t){var e=t.children;return n.createElement(n.Fragment,{},e)}},m=n.forwardRef((function(t,e){var a=t.components,r=t.mdxType,l=t.originalType,d=t.parentName,p=o(t,["components","mdxType","originalType","parentName"]),u=s(a),m=r,g=u["".concat(d,".").concat(m)]||u[m]||k[m]||l;return a?n.createElement(g,i(i({ref:e},p),{},{components:a})):n.createElement(g,i({ref:e},p))}));function g(t,e){var a=arguments,r=e&&e.mdxType;if("string"==typeof t||r){var l=a.length,i=new Array(l);i[0]=m;var o={};for(var d in e)hasOwnProperty.call(e,d)&&(o[d]=e[d]);o.originalType=t,o[u]="string"==typeof t?t:r,i[1]=o;for(var s=2;s<l;s++)i[s]=a[s];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},2312:function(t,e,a){a.r(e),a.d(e,{assets:function(){return p},contentTitle:function(){return d},default:function(){return g},frontMatter:function(){return o},metadata:function(){return s},toc:function(){return u}});var n=a(7462),r=a(3366),l=(a(7294),a(3905)),i=["components"],o={id:"datastacks",title:"Datastacks overview",sidebar_label:"Datastacks",hide_title:!1,hide_table_of_contents:!1,description:"Overview of the Datastacks utility",keywords:["data","python","etl","cli","azure","template"]},d=void 0,s={unversionedId:"workloads/azure/data/etl_pipelines/datastacks",id:"workloads/azure/data/etl_pipelines/datastacks",title:"Datastacks overview",description:"Overview of the Datastacks utility",source:"@site/docs/workloads/azure/data/etl_pipelines/datastacks.md",sourceDirName:"workloads/azure/data/etl_pipelines",slug:"/workloads/azure/data/etl_pipelines/datastacks",permalink:"/docs/workloads/azure/data/etl_pipelines/datastacks",draft:!1,tags:[],version:"current",frontMatter:{id:"datastacks",title:"Datastacks overview",sidebar_label:"Datastacks",hide_title:!1,hide_table_of_contents:!1,description:"Overview of the Datastacks utility",keywords:["data","python","etl","cli","azure","template"]},sidebar:"docs",previous:{title:"ETL Overview",permalink:"/docs/workloads/azure/data/etl_pipelines/etl_intro_data_azure"},next:{title:"Data Ingest Workloads",permalink:"/docs/workloads/azure/data/etl_pipelines/ingest_data_azure"}},p={},u=[{value:"Using the Datastacks CLI",id:"using-the-datastacks-cli",level:2},{value:"Generating data workloads",id:"generating-data-workloads",level:2},{value:"Commands",id:"commands",level:3},{value:"Examples",id:"examples",level:3},{value:"Configuration",id:"configuration",level:3},{value:"All workloads",id:"all-workloads",level:4},{value:"Ingest workloads",id:"ingest-workloads",level:4},{value:"Processing workloads",id:"processing-workloads",level:4}],k={toc:u},m="wrapper";function g(t){var e=t.components,a=(0,r.Z)(t,i);return(0,l.kt)(m,(0,n.Z)({},k,a,{components:e,mdxType:"MDXLayout"}),(0,l.kt)("p",null,"Datastacks provides a suite of utilities built to accelerate development within an Ensono Stacks Data Platform, supporting common tasks such as generating new data engineering workloads and running Spark jobs. Datastacks consists of:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#using-the-datastacks-cli"},"Datastacks CLI")," - A command-line interface for data engineers, enabling interaction with Datastacks' various functions."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"#generating-data-workloads"},"Data workload generation")," - Generate new data workloads based upon common templates."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/etl_pipelines/pyspark_utilities"},"PySpark utilities")," - A suite of reusable utilities to simplify development of data pipelines using Apache Spark and Python."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/etl_pipelines/data_quality_azure"},"Data quality utilities")," - Utilities to support the data quality framework implemented in Stacks."),(0,l.kt)("li",{parentName:"ul"},"Azure utilities - Utilities to support common interactions with Azure resources from data workloads."),(0,l.kt)("li",{parentName:"ul"},"Behave utilities - Common scenarios and setup used by ",(0,l.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/etl_pipelines/testing_data_azure#end-to-end-tests"},"Behave end-to-end tests"),".")),(0,l.kt)("h2",{id:"using-the-datastacks-cli"},"Using the Datastacks CLI"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/getting_started/dev_quickstart_data_azure"},"Setup project environment"))),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"# Option 1: Run Datastacks CLI using Poetry's interactive shell (recommended for local development)\npoetry shell\ndatastacks --help\n\n# Option 2: Run Datastacks CLI using poetry run (recommended where Poetry shell cannot be used, e.g. CI/CD pipelines)\npoetry run datastacks --help\n")),(0,l.kt)("h2",{id:"generating-data-workloads"},"Generating data workloads"),(0,l.kt)("p",null,"Datastacks can be used to generate all the resources required for a new data engineering workload - for example a ",(0,l.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/etl_pipelines/ingest_data_azure"},"data ingest")," or ",(0,l.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/etl_pipelines/data_processing"},"data processing")," pipeline. This will create all resources required for the workload, based upon templates within the ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/ensono/stacks-azure-data/tree/main/de_templates"},"de_templates")," directory."),(0,l.kt)("p",null,"The ",(0,l.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/architecture/architecture_data_azure#data-engineering-workloads"},"deployment architecture")," section shows the workflow for using Datastacks to generate a new workload.\nThe ",(0,l.kt)("a",{parentName:"p",href:"/docs/workloads/azure/data/getting_started/"},"getting started")," section includes step-by-step instructions on deploying a new ingest or processing workload using Datastacks."),(0,l.kt)("h3",{id:"commands"},"Commands"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"strong"},"generate")),": Top-level command for generating resources for a new data workload.",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"strong"},"ingest")),": Subcommand to generate a new ",(0,l.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/etl_pipelines/ingest_data_azure"},"data ingest workload"),", using the provided configuration file. A optional flag (",(0,l.kt)("inlineCode",{parentName:"li"},"--data-quality")," or ",(0,l.kt)("inlineCode",{parentName:"li"},"-dq"),") can be included to specify whether to include data quality components in the workload."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"strong"},"processing")),": Subcommand to generate a new ",(0,l.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/etl_pipelines/data_processing"},"data processing workload"),", using the provided configuration file. A optional flag (",(0,l.kt)("inlineCode",{parentName:"li"},"--data-quality")," or ",(0,l.kt)("inlineCode",{parentName:"li"},"-dq"),") can be included to specify whether to include data quality components in the workload.")))),(0,l.kt)("h3",{id:"examples"},"Examples"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'# Activate virtual environment\npoetry shell\n\n# Generate resources for an ingest workload\ndatastacks generate ingest --config="de_templates/test_config_ingest.yaml"\n\n# Generate resources for an ingest workload, with added data quality steps\ndatastacks generate ingest --config="de_templates/test_config_ingest.yaml" --data-quality\n\n# Generate resources for a processing workload\ndatastacks generate processing --config="de_templates/test_config_processing.yaml"\n\n# Generate resources for a processing workload, with added data quality steps\ndatastacks generate processing --config="de_templates/test_config_processing.yaml" --data-quality\n')),(0,l.kt)("h3",{id:"configuration"},"Configuration"),(0,l.kt)("p",null,"In order to generate a new data engineering workload the Datastacks CLI takes a path to a config file. This config file should be YAML format, and contain configuration values as specified in the table below. Sample config files are included in the ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/ensono/stacks-azure-data/tree/main/de_templates"},"de_templates")," folder. The structure of config is ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/ensono/stacks-azure-data/tree/main/datastacks/datastacks/config.py"},"validated using Pydantic"),"."),(0,l.kt)("h4",{id:"all-workloads"},"All workloads"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Config field"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"),(0,l.kt)("th",{parentName:"tr",align:null},"Required?"),(0,l.kt)("th",{parentName:"tr",align:null},"Format"),(0,l.kt)("th",{parentName:"tr",align:null},"Default value"),(0,l.kt)("th",{parentName:"tr",align:null},"Example value"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"pipeline_description"),(0,l.kt)("td",{parentName:"tr",align:null},"Description of the pipeline to be created. Will be used for the Data Factory pipeline description."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},'"Ingest from demo Azure SQL database using ingest config file."')),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ado_variable_groups_nonprod"),(0,l.kt)("td",{parentName:"tr",align:null},"List of required variable groups in non-production environment."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"List","[String]"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"- amido-stacks-de-pipeline-nonprod",(0,l.kt)("br",null),"- stacks-credentials-nonprod-kv")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ado_variable_groups_prod"),(0,l.kt)("td",{parentName:"tr",align:null},"List of required variable groups in production environment."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"List","[String]"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"- amido-stacks-de-pipeline-prod",(0,l.kt)("br",null),"- stacks-credentials-prod-kv")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"default_arm_deployment_mode"),(0,l.kt)("td",{parentName:"tr",align:null},"Deployment mode for terraform."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},'"Incremental"'),(0,l.kt)("td",{parentName:"tr",align:null},"Incremental")))),(0,l.kt)("h4",{id:"ingest-workloads"},"Ingest workloads"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Config field"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"),(0,l.kt)("th",{parentName:"tr",align:null},"Required?"),(0,l.kt)("th",{parentName:"tr",align:null},"Format"),(0,l.kt)("th",{parentName:"tr",align:null},"Default value"),(0,l.kt)("th",{parentName:"tr",align:null},"Example value"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"dataset_name"),(0,l.kt)("td",{parentName:"tr",align:null},"Dataset name, used to derive pipeline and linked service names, e.g. AzureSql_Example."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"azure_sql_demo")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"data_source_password_key_vault_secret_name"),(0,l.kt)("td",{parentName:"tr",align:null},"Secret name of the data source password in Key Vault."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"sql-password")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"data_source_connection_string_variable_name"),(0,l.kt)("td",{parentName:"tr",align:null},"Variable name for the connection string."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"sql_connection")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"data_source_type"),(0,l.kt)("td",{parentName:"tr",align:null},"Data source type."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"String",(0,l.kt)("br",null),(0,l.kt)("br",null),"Allowed values",(0,l.kt)("sup",{parentName:"td",id:"fnref-1"},(0,l.kt)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1")),":",(0,l.kt)("br",null),'"azure_sql"'),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"azure_sql")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"bronze_container"),(0,l.kt)("td",{parentName:"tr",align:null},"Name of container for landing ingested data."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},"raw"),(0,l.kt)("td",{parentName:"tr",align:null},"raw")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"key_vault_linked_service_name"),(0,l.kt)("td",{parentName:"tr",align:null},"Name of the Key Vault linked service in Data Factory."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},"ls_KeyVault"),(0,l.kt)("td",{parentName:"tr",align:null},"ls_KeyVault")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"trigger_start"),(0,l.kt)("td",{parentName:"tr",align:null},"Start datetime for Data Factory pipeline trigger."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"Datetime"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"2010-01-01T00:00:00Z")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"trigger_end"),(0,l.kt)("td",{parentName:"tr",align:null},"Datetime to set as end time for pipeline trigger."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"Datetime"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"2011-12-31T23:59:59Z")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"trigger_frequency"),(0,l.kt)("td",{parentName:"tr",align:null},"Frequency for the Data Factory pipeline trigger."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"String",(0,l.kt)("br",null),(0,l.kt)("br",null),"Allowed values:",(0,l.kt)("br",null),'"Minute"',(0,l.kt)("br",null),'"Hour"',(0,l.kt)("br",null),'"Day"',(0,l.kt)("br",null),'"Week"',(0,l.kt)("br",null),'"Month"'),(0,l.kt)("td",{parentName:"tr",align:null},'"Month"'),(0,l.kt)("td",{parentName:"tr",align:null},"Month")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"trigger_interval"),(0,l.kt)("td",{parentName:"tr",align:null},"Interval value for the Data Factory pipeline trigger."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"Integer"),(0,l.kt)("td",{parentName:"tr",align:null},"1"),(0,l.kt)("td",{parentName:"tr",align:null},"1")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"trigger_delay"),(0,l.kt)("td",{parentName:"tr",align:null},"Delay between Data Factory pipeline triggers, formatted HH:mm:ss"),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},'"02:00:00"'),(0,l.kt)("td",{parentName:"tr",align:null},"02:00:00")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"window_start_default"),(0,l.kt)("td",{parentName:"tr",align:null},"Default window start date in the Data Factory pipeline."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"Date"),(0,l.kt)("td",{parentName:"tr",align:null},'"2010-01-01"'),(0,l.kt)("td",{parentName:"tr",align:null},"2010-01-01")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"window_end_default"),(0,l.kt)("td",{parentName:"tr",align:null},"Default window end date in the Data Factory pipeline."),(0,l.kt)("td",{parentName:"tr",align:null},"No"),(0,l.kt)("td",{parentName:"tr",align:null},"Date"),(0,l.kt)("td",{parentName:"tr",align:null},'"2010-01-31"'),(0,l.kt)("td",{parentName:"tr",align:null},"2010-01-31")))),(0,l.kt)("h4",{id:"processing-workloads"},"Processing workloads"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Config field"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"),(0,l.kt)("th",{parentName:"tr",align:null},"Required?"),(0,l.kt)("th",{parentName:"tr",align:null},"Format"),(0,l.kt)("th",{parentName:"tr",align:null},"Default value"),(0,l.kt)("th",{parentName:"tr",align:null},"Example value"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"pipeline_name"),(0,l.kt)("td",{parentName:"tr",align:null},"Name of the data pipeline / workload."),(0,l.kt)("td",{parentName:"tr",align:null},"Yes"),(0,l.kt)("td",{parentName:"tr",align:null},"String"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("em",{parentName:"td"},"n/a")),(0,l.kt)("td",{parentName:"tr",align:null},"processing_demo")))),(0,l.kt)("div",{className:"footnotes"},(0,l.kt)("hr",{parentName:"div"}),(0,l.kt)("ol",{parentName:"div"},(0,l.kt)("li",{parentName:"ol",id:"fn-1"},"Additional data source types will be supported in future - see ",(0,l.kt)("a",{parentName:"li",href:"/docs/workloads/azure/data/etl_pipelines/ingest_data_azure#data-source-types"},"ingest data source types"),".",(0,l.kt)("a",{parentName:"li",href:"#fnref-1",className:"footnote-backref"},"\u21a9")))))}g.isMDXComponent=!0}}]);